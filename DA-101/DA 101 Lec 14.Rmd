---
title: "DA 101 Lec 14"
author: "Your Name"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    code_folding: show
    theme: simplex
    df_print: paged
  
---

```{r setup, include=FALSE}

#---------- RSTUDIO STARTER V 2.0  --------------#
#                    -Prepared with care by  AM ;D
                
                                                                          
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE)      
library(tidyverse)                
library(ggthemes)                    

theme_set(theme_tufte() +                                     
  theme(text=element_text(family="sans")))  

#------------------------------------------------#
```


## Linear Regression Continued - Evaluating our Models

Today, we'll investigate two important tools for evaluating our model:  "how good is it?"

Recall, we've been investigating the relationship between a car's engine displacement (size) and its fuel efficiency (cty mpg).

Always, use your human eyeballs first:  scatterplot:

```{r}
qplot(mpg$displ, mpg$cty, geom="point")+geom_smooth(method="lm")
```

Observations:  seems like a linear model is reasonable.  

What's r, the correlation coefficient?

```{r}
cor(mpg$displ, mpg$cty)
```

A correlation coefficient of -.8 indicates a fairly strong negative linear relationship.  Spoiler alert:  today, we'll do a hypothesis test for r!

What can the equation of the line tell us?  Let's make an lm.  Here, we think of fuel efficiency as a function of displacement.  In other words, x is displacement, y is fuel efficiency.  Our model:  y ~ x

```{r}
mpgModel <- lm(mpg$cty ~ mpg$displ)

summary(mpgModel)

```

Here, yhat = 25.9915 - 2.6305x

Or, yhat = -2.6305x + 25.9915.

So, what do these coefficients actually mean?

Slope:  In general, if x increases by 1 unit, we expect y to change by the slope.  Here, if the engine size (displ) increases by 1L, then we expect its city fuel efficiency to decrease by 2.6305mpg.  

Intercept:  In general, if x is zero, we expect y to be the y-intercept.  Here: if a car's engine is zero L, we expect its fuel efficiency to be 25.9915 (this is silly, extrapolation!)  Engineers say:  junk in, junk out!

One more key coefficient:  the coefficient of determination R^2.  In general, R^2 is the percentage of variation in y data that's because of the linear relationship between x and y.

Here:  About 63.76% of the variation in fuel efficiency is because of the linear rel between fuel efficiency and displ.  We know that displ isn't the only factor.  How much does it account for?  About 63.76%.

## Part II: Evaluating the Model

There are a couple of key ways to evaluate how good a model is.

### Residual plot

Recall: residuals are the error in prediction for each point.  I.e., the vertical distance between the point and the line.

Also recall: the equation of the least square line is the one that minimizes the square residuals.  We square them to prevent cancelling and get a cumulative measure of error.

Looking at the residuals can give an important measure of how good the prediction are.   It's called a residual plot:  x axis is the x-vals of the data, y-axis is the RESIDUALS (not the y data).

R stores all the resids as a variable in the lm:

```{r}

#plot the resids:
qplot(mpg$displ, mpgModel$residuals, geom="point")+xlab("displacement")+
  ylab("residuals")
```

What are we looking for?  Couple things:

1) We don't want to see a pattern.  If there's a pattern in the error, if our error is predictable, then we ought to be able to make a better model!

- What patterns do we see?
  - Polynomial/parabola.  Seen above!
  - Homoskedasticity.  
  
If we do see a pattern (curve or not homosked), it indicates that some other model could do a better job!  Pattern means lin model not the best!

For the car example above, I'm not too worried about homosked, but I am worried about the curve/polynomial pattern.  Seems like there could be a better model.

Another quality of our residuals:  we hope that they're roughly normally distributed (bell curve).  It's easy to make a histo:

```{r}
qplot(mpgModel$residuals, geom="histogram", bins=10)
```

Here, looks like the resids do follow a normal dist.  Close enough.

### Hyp Tests

There's a hyp test for the correlation coefficient r.  Students often ask:  how big is "big enough"??  How close does r have to be to +/- 1 in order to say that there's a lin rel?

To answer this question, we use p-vals!

```{r}
cor.test(mpg$displ, mpg$cty)
```

Let's interpret:

1)  Hypotheses

H0:  true correlation = 0 (rho = 0)
Ha:  true correlation != 0 (rho != 0)

Note, cor tests have a big limitation.  If we reject the null hypothesis, we've found evidence that rho != 0.  The means there is evidence for at least *some* lin rel, but it doesn't tell you how strong that relationship is!!!!

Cor test are useful for showing that two variables *are related*, but they don't tell you how strongly related they are.  You'll need look at other things (like resids, coefficients, etc) to decide how reliable your predictions are.

2) Test stat is a t test stat just like before.  It's a summary of the data, i.e., the correlation coeff.  The df is basically the sample size, in this case:  df = n-2.  Here, n=234.

3) P-val is the same as always.  Here, the chance of observing a correlation coefficient r that's as large as it is (given that the true correlation is zero) is .000000000000000216.  Very unlikely!  So, we reject H0.

4)  Conclusion:  same as always:  if p is small, we reject the null hyp.  Here, we've found strong evidence that the true correlation is nonzero; i.e., there's at least some relationship between displ and mpg.

Office hours:  cancelled today.  Instead:  tomorrow 10-12.


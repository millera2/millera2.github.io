---
title: "DA 101 Lec 18"
author: "Your Name"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    code_folding: show
    theme: simplex
    df_print: paged
  
---

```{r setup, include=FALSE}

#---------- RSTUDIO STARTER V 2.0  --------------#
#                    -Prepared with care by  AM ;D
                
                                                                          
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE)      
library(tidyverse)                
library(ggthemes)                    

theme_set(theme_tufte() +                                     
  theme(text=element_text(family="sans")))  

#------------------------------------------------#
```

## Multiple Lin Reg Continued (Again): Coefficients

The (most?) important part model interpretation is in interpreting the coefficients.

For linear models, there's two: 

- the intercept
- slope

Interpreting coefficients for multiple regression can be hairy.  In theory, it's the same as for single variable, but in practice, things can get kinda weird.

Let's look at an example, iris.

```{r}
irisMod <- lm(data=iris, Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width)
summary(irisMod)
```

Interpret coeffs:

If an iris sepal width increases by 1cm, we expect sepal length to increase by .65cm.

If an iris petal width increases by 1cm, we expect sepal length to decrease by .55648.

Sanity check:  does that make sense?  Let's take a look with our human eyeballs

```{r}
# plot, x= petal width, y = sepal length

iris %>% ggplot(aes(x=Petal.Width, y=Sepal.Length))+geom_point()+geom_smooth(method="lm")
```

Wuut?  Doesn't make sense!  We can clearly see that there's a positive relationship between Petal width and sepal lenght.  BUT, in our multiple linear model, the coefficient for Petal Width is negative!!!

Moral of the story:  the coefficients in multiple linear models can get a little crazy, and they're not always a reliable measure of the relationship for specific variables.

```{r}
petalMod <- lm(data=iris, Sepal.Length~Petal.Width)
summary(petalMod)

```


Conclusion:  multi linear models are great for *predictive power*.  If your goal is to make the best predictions you can, then use a multi linear model.

BUT, if your goal is to understand the relationship between your variables, then single-variable variable models are better since their coefficients make sense!

This is why, in lab 6, Dr Brady advised you to make both single-variable and multi-variable models.  


## Regression with Categorical Variables

At fist, it doesn't seem to make much sense.  How can you predict a number based on a category?  How can you plug in a category into a linear model?  

There is a strategy, and it's called "dummy coding".  Let's look at an example:

```{r}
speciesMod <- lm(data=iris, Sepal.Length~Species)
summary(speciesMod)

```


What's going on?  Here's how it works:  Each level of the category (setosa, versicolor, virginica) gets a "dummy variable", either 0 or 1.

If zero, then NOT in category.  If 1, then yes in the category.  Our equation looks like this:

Example, if a plan is versicolor, that means that x1 = 1 and x2 = 0 

What do the coefficients mean?  Let's look at the data.  First, filter by species:

```{r}
setosaData <- iris %>% filter(Species=="setosa")
versicolorData <- iris %>% filter(Species=="versicolor")
virginicaData <- iris %>% filter(Species =="virginica")

#look at mean sepal length:

mean(setosaData$Sepal.Length)

```

Explanation:  if a plan is setosa, the model just predicts the mean for the setosas.

What about versicolor?

```{r}
mean(versicolorData$Sepal.Length)
```

Explanation: the versicolor coefficient shows how much change relative to setosa.  

In other words, setosa got baked into the model as a "baseline".  The other coefficients tell us how much change relative to setosa.

For virginica, guess:

```{r}
5.006+1.5820
```

Is this correct?

```{r}
mean(virginicaData$Sepal.Length)
```

In summary, categorical variales affect the intercept, a vertical shift that depdends on the mean for the category.  One of the variables gets "baked in" as a "baseline" and becomes the intercept for the model.

You can combine categorical and numerical:

```{r}
mixedMod <- lm(data=iris, Sepal.Length ~ Sepal.Width+Species)

summary(mixedMod)
```

Here we get 3 models, one for each species, slope is determined by Sepal Width (numeral one!).
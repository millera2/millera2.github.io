---
title: "Math 120 Lec 06 "
author: "Ali Miller"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    code_folding: show
    theme: simplex
    df_print: paged
  
---

```{r setup, include=FALSE}
###########################################################################
################# DON'T DELETE THIS PART!!!!!##############################
###########################################################################
                                                                          #
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)      #
                                                                          #
library(tidyverse)           #The tidyverse has all of our data tools     #
library(ggthemes)            #ggthemes makes our graphs look nicer        #
                                                                          # 
theme_set(                                #this sets the theme for all    #
  theme_tufte() +                         #plots that we make             #
  theme(text=element_text(family="sans"))                                 #
  )                                                                       #
                                                                          #
                                                                          #
                                                                          #
### Below is where you read and store your data.                          #
                                                                          #
youShouldRenameThis <- read_csv("death penalty data.csv")
                                                                          #
                                                                          #
                                                                          #
############# ^ ################################# ^ #######################
############# | ## DON'T DELETE THIS PART!!!!!### | #######################
############# | ################################# | #######################
```

## Reading Discussion: Linear Regression

##Intro to Correlation and Linear Regression


We start with a scatterplot, an x-y plot of two numerical variables.

### Examples: mpg data


```{r}

mpg %>% head


#examine rel btwn cty and hwy

qplot(x=cty, y=hwy, data=mpg)

```

One more:  displ vs cty.

```{r}


qplot(displ, cty, data=mpg, geom="point")
```




### Key qualities of a linear relationship

1) Form/shape.  Here in 120, we only do linear relationships.  But, there are many kinds:  exponential, logistic, polynomial, etc.  

2) Direction:  pos or neg.

In examples, the first cty vs hwy is positive.  Both go up together!  However, displ vs cty is a negative rel:  if displ gets bigger, cty gets smaller.


3) Strength.  In our examples, cty/hwy is stronger than displ/cty.

## Linear correlation coefficient r


Note: this little "r" is different from our big "R".  

The correl coefficient r measures the strength and direction of our relationship.

If r is close to 1, strong pos rel.

If r is close to -1, strong neg rel.

If r is close to zero, there's not a linear relationship.  

Warning!  r only describes linear relationships.  Ex on board:

##### Properties of r

-   -1 < r < 1    always!!!!!  (not possible to have r=1.3)
- doesn't matter which is x, which is y
- r doesn't depend on units (i.e. feet/meters)  (whew! don't have to worry about data units!!)






### Example: 8.8 on page 314

Identifying r in graphs.

## Least Squares Equation




### The equation

It's a line!  In high school, you wrote:

y = mx+b

In stats, we use different letters:

yhat = b_1 *x + b_0

b_1 is the slope (like m)

b_0 is the intercept (like b)

Exactly the same!


#### Example: mpg data

```{r}

# in R, we use lm for linear model.

# here's how to find equation for cty and hwy in mpg:



lm(hwy~cty, data=mpg)



mpg %>% head

```

Observations:  

- The ~ has a special meaning:  here, hwy is a function of cty.  Note that y comes first:  here, that's hwy fuel efficiency.
- I.e., we're predicting hwy based on cty.


Let's visualize:

```{r}

qplot(cty, hwy, data=mpg)+geom_smooth(method = "lm")

#what's R?  use cor()

cor(mpg$cty, mpg$hwy)

```

Challenge:  plot displ vs cty, find correlation coeff, and plot the line

```{r}
qplot(displ, cty, data=mpg)+geom_smooth(method="lm")

lm(cty ~ displ, data=mpg)

cor(mpg$cty, mpg$displ)
```


### Uniqueness

The least-square line is unique, i.e., *the best* linear model.

This is a releif, we don't have to worry about *which line*!!

Names:

- Least square line
- The regression line
- The linear model
- The best fit line
- Other things that sound like that

### Idea behind deriving it

The vertical distance (between line and points) represents error!  (Btw, it's called a residual.)

The least square line is the one that makes the square error as small as possible.  

Question:  why minimize square error:

(y-yhat)^2

Some residuals are neg, some are pos.  We don't want them to cancel out!  So, we square to get all positive terms.

The least squares line minimizes the square of the residuals.  I.e., smallest square error.

### Using lm in R

We've seen lm before.  Example:  cty and hwy mpg.

```{r}

lm(hwy ~ cty, data=mpg)


```

The lm object has lots of cool properties about the model.  Save it!  Give it a name!

```{r}

lm(hwy ~ cty, data=mpg) -> mpgModel

#neat things:  summary

summary(mpgModel)

```


### Making predictions

R has built-in prediction thingies, but don't use them:  it's easy!

Ex:  Using this model, predict the hwy fuel efficiency of a car that gets 30mpg in the city.

Easy to just use R as a calculator:

```{r}

# predict yhat when x is 30

#plug in to least sqaures:


.892+1.337*30


```

Following the linear model, we predict that a car with 30mpg in city would get 41.002 mpg on hwy.

What about if our car got 25mpg in city?

```{r}
.892 + 1.337*25
```


Katie's question:  wait, what's up with x and y?

X is the explanatory variable, we think of it as "causing" an effect in Y.  

Y is the response variable, i.e., what we're making predictions about.

Here, in our graph, can see that we're using `cty` as x and `hwy` as Y.

Thus, we're predicting hwy mpg based upon cty mpg.

### Meaning of the coefficients

You can now get coeffs from lm.  But what do they **mean**?

First:  slope = b_1.  Slope tells you the amount of change in y for each unit change in x.

In other words:  if x gets bigger by 1, then we expect y to change by [slope].

x: cty fuel efficiency in mpg
y: hwy fuel efficiency in mpg

All together:  If a car's city fuel efficiency increases by 1mpg, we expect its highway fuel efficiency to increase by 1.337mpg.

2nd:  b_0 = the intercept

The intercept is our prediction when x=0.

If a car has a city mpg of zero, we expect its highway mpg to be 0.892.  Notice:  this doesn't make sense!  Note: this is an example of "extrapolation", since x=0 is unreasonable.


Extrapolation is the process of using "unreasonable" x values to predict y.  "Unreasonable" is up to your good human judgement.  More on that later.


## Residuals

Residuals are:
- e = (y - yhat)
- the vertical distance between points and the line
- the error in our precition!  We hope they're small!

In order to see if we have a "good" model, we need to analyze these.  

Seeing these in R is super easy:

```{r}


summary(mpgModel)




#the model thingie contains all the residuals as a variable.

#mpgModel$residuals

#we always want to look:  residual plot.


mpg %>% head

#in residual plot, x comes from original data, y is the resids from your lm model

qplot(mpg$cty, mpgModel$residuals)

```

On average, the residuals are zero.  Makes sense, because pos and neg balance out.




### Meaning (math and graphical)

### Example: mpg data

### Interpreting Residual Plots

By analyzing residual plots, we can find more evidence for (or against) the linear model.

Key feature:  PATTERNS!

If residuals have -no- pattern, then there's no way to improve the model.  I.e., linear model is good!

If residuals follow a visible pattern, it suggests that there could be a better model than lm.



## Coefficient of Determination R^2

The symbol is R^2 or r^2.  Btw, it really is (r)^2.

R^2 tells us something special:  it's the percent of variation in the y data that because of the linear rel with the x data.

Eg: mpg:  displ (x) vs (cty) (y)

```{r}
myModel <- lm(cty~displ, data=mpg)

summary(myModel)

qplot(displ, cty, data=mpg)
```

Here, r^2 is .6376.  Woah!

Even though we know that displacement (i.e. size) of the engine isn't the only thing affecting city mpg, it accounts for about 63.76% of the variation we see in city mpg.




##Effects of Outliers
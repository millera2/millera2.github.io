---
title: "Math 120 Week 8"
author: "Ali Miller"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    code_folding: show
    theme: yeti
    df_print: paged
  
  
---

```{r setup, include=FALSE}

#---------- RSTUDIO STARTER V 2.0  --------------#
#                    -Prepared with care by  AM ;D
                
                                                                          
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE)      
library(tidyverse)                
library(ggthemes)                    

theme_set(theme_tufte() +                                     
  theme(text=element_text(family="sans")))  

studentSurvey <- read_csv("Student Data.csv")
deathPenaltyData <- read_csv("death penalty data.csv")
housingData <- read_csv("../Data/House-Sales.csv")
#------------------------------------------------#
```






# Monday 10-19

## Ch 5:  Inference

Recall, **statistical inference** is the process of making mathematically supported conclusions about populations based upon sample data.


Statistical inference always starts with a "point estimate", ie a single number that represents our best guess about the population paramater.

  (Rember:

  - Parameters are summaries of populations
  - Statistics are summaries of samples

  )

You already know a couple of point estimates:

-xbar is a point estimate for a population mean, mu
-phat is a point estimate for a population proportion, p

Two problems:

-No guarantee that these are correct for populations!  Sample are random!
- (So far) we don't know how much variability there is in these point estimates.  

Let's study both of these with an example from the text (p170):

- Given the parameter:  p = .88  (in real life, we wouldn't know!)

From some of our example random samples of size 1000, we see the resulting proportion varies.  We saw:  86.1%, 87.8%, 90.1%, 89.4%, etc etc.

Idea:  let's perform this sample 5000 times and look at the results.  We are going to make a histogram of 5000 sample proportions from 5000 samples.  

Observations from the histogram:

- Looks like sample proportions follow a normal dist (ooh!)
- The center of the distribution is .88 (most are close to the true value).
- We have an idea for the variability of the sample results.  Here, they all seem to be within +/- 4% of the true value.

## Sampling Distributions

The histogram you saw above is an example of a "sampling dist", ie the distribution for possible SAMPLE RESULTS.

Restate:  a sampling distribution is a probability distribution that describes the likelihood of observing a sample result of fixed size, n (i.e. it DOESN'T tell you about individuals!)

As you've seen, these often look like normal dists!  This amazing fact is called the **Central Limit Theorem**.  

According to the CLT, sample statistics (like xbar and phat) will always follow a normal distribution as long as the sample size is "large enough".

For **sample proportions**, phat,  they follow a normal dist with:

1) mean = p

$$\mu_{\hat{p}}=p$$

In other words, the expected value for sample proportions phat is the true population proportion, p.

2) stdev = sqrt(p*(1-p)/n)

$$\sigma_\hat{p}=\sqrt{p(1-p)/n}$$

Thus, for any particular sample proportion, phat, we can find z-scores!

For phat:

     z = (phat - p)/sqrt(p*(1-p)/n)
       = (obs - exp)/stdev

























# Friday Oct 23

Recall: sampling distributions are probability distributions for entire sample results.

The goal:  answer the question, "how likely (or unlikely) is our sample result?

Remember, the only reason this is possible is because of the CENTRAL LIMIT THEOREM: ie, if our sample size is "large enough", we know that our sample results (xbar and phat) follow a normal distribution.

Wait, what's "large enough?"  Answer: it depends!

First, we'll think about categorical variables (phat).  
Later, we'll think about quantitative variables (xbar).  

For now, "large enough" for categorical variables means the following:

    n*phat >= 10   n*(1-phat) >= 10
    
Equivalently, we need at least 10 individuals that ARE IN the category, and at least 10 indivuals that ARE NOT IN the category.  


**Ex**  In our class, 65% of students have brown hair.  There are 50 students (n) in our class.  Could we use a normal distribution to describe the likelihood of such a scenario?

Check:

```{r}
50*.65
```

```{r}
50*.35
```

Yes!  n is "large enough"!

**Ex**  Suppose that 60% of all DU students have brown hair.   How likely would it be to take a sample of 50 students, and find that 65% (or more) of those students have brown hair?


We saw above that n is "large enough"

mu_phat = mean = .6 (population proportion)
sigma_phat = stdev = sqrt(.6*(1-.6)/50)

```{r}
sqrt(.6*(1-.6)/50)
```

obs = .65
exp = .60
stdev = .0693

    z = (obs - exp)/stdev
      = (.65 - .6)/.0693
      

```{r}
(.65 - .6)/.0693
```

z = .72.  Look it up!


```{r}
1-0.7642
```

There's about a 23.58% chance that in a samle of 50 students, 65% or more would have brown hair.

Such an occurrance is not unlikely.


**Ex**  According to the internet, 82% of DU students are from "out-of-state" (not from Ohio).  

If we took a sample of 100 students, what's the probability that at least 90% of them are from out-of-state?

Check to see if we can use normal dist:


```{r}
100*.9
```

```{r}
100*(1-.9)
```

Close!  But, both are at least 10, so we CAN use the normal dist!

obs = .9
exp = .82
stdev = sqrt(.82*(1-.82)/100)


```{r}
sqrt(.82*(1-.82)/100)
```

Find zscore!


```{r}
(.9 - .82)/.03842
```

z= 2.08



```{r}
1-0.9812
```

There's about a 1.88% chance of obtaining such a sample result.  Pretty unlikely!
















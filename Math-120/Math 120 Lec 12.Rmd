---
title: "Math 120 Lec 12"
author: "Your Name"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    code_folding: show
    theme: simplex
    df_print: paged
  
  
---

```{r setup, include=FALSE}

#---------- RSTUDIO STARTER V 2.0  --------------#
#                    -Prepared with care by  AM ;D
                
                                                                          
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE)      
library(tidyverse)                
library(ggthemes)                    

theme_set(theme_tufte() +                                     
  theme(text=element_text(family="sans")))  

#------------------------------------------------#
```

## Sampling With Replacement

#### Ex: Bag of Marbles

A bag of 20 marbles has 12 blue, 8 red marbles.  If you draw three marbles in a row, what's the chance that all three are blue?


First marble:  12/20 chance of blue
2nd marble:  11/19
3rd marble: 10/18

All together:  (12/20)*(11/19)*(10/18) = whatev

It was important that we didn't replace the marbles.

Q:  How would it be different if we *did* replace the marbles?

A:  The probabilities don't change, these become independent events!

P(all thee blue) = (12/20)*(12/20)*(12/20)

The issue here is sampling with replacment.

Replacement means "independent", since each outcome doesn't affect the others.

Q:  How different are they?

```{r}

(12/20)*(11/19)*(10/18)

(12/20)*(12/20)*(12/20)

```

There's about a 3 percent difference.  Not huge, but substantial.

Q:  How would the math work if we had 20000 marbles?

```{r}
# compare replacement vs not replacement

#first, no replacement

(12/20000)*(11/19999)*(10/19998)

#with replacement

(12/20000)*(12/20000)*(12/20000)
```
No replacment:  P = .000000000165
Replacement:    P = .000000000216

Matt's observation:  as population size increases, the difference beteween replacement and not replacement gets very small. 

Ie, if we've got a big population, **we don't care** if we're replacing or not!  They're basicaly the same!

#### Moral of the story

We assume independence whenever our population is large relative to the sample size:  

A common rule of thumb, the sample should be no more than 5% of the population size.

Ex:  If our sample is size 5, and pop is size 100, we assume independence.

**NOTE**  Lab 3 #2!!!!!

## Discrete Random Variables

A discrete random variable assigns a **real number** to every outcome in a probability experiment.

In order to do math with this stuff, we need numerical represenations.


## Discrete Probability Distributions

A discrete distro is a discrete random variable, along with the probabilities for each.

Usually, we represent these as tables.

### Simple Example: Rolling a single dice

Scenario:  we roll a dice.  The discrete random variable (RV) X is the face that shows.

Outcomes for X:  1, 2, 3, 4, 5, 6

Here's how to do this in R

```{r}
X1 = c(1, 2, 3, 4, 5, 6)

P1 = c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6)

```




## Expected Value of a discrete distro

An important quality of a distribution:  the "expected" value, i.e., the thing we expect in the long run.

It's a weighted average of all the outcomes of X, weighted by probability.

### Definition

Formula:  E(X) = sum(x*P(x))


### Example: single dice

This is super easy to find in R:

```{r}
X1
P1

#Find expected value:

sum(X1*P1)

```

Q:  When you roll a dice, how frequently do you get a 3.5?

**Arya's observation**:  isn't it like average?

**Prof Miller's reply**:  isn't average different every time?

Connected to Law of Large numbers:  as sample size increases, observed proporiton gets closer to the true probability.

Here, we're not talking individual outcomes, we're talking about **whole samples**!  Means!

If we take larger and larger samples of dice rolls, we expect our average to get closer and closer to 3.5.  

```{r}
#sample 10000 dice rolls

diceSample <- sample(1:6, 10000, replace = TRUE)


mean(diceSample)

```

Observation:  large sample size has average that's close to the expected value.  If n increases, it "generally" gets closer still.

#### 3.30a Expected Value

```{r}
X = c(0,5,10,20)
P = c(26/52, 13/52, 12/52, 1/52)


#Expected Value:
sum(X*P)

```

What does this mean?  

It **doesn't** mean:

- We're not ever going to get 3.94.  Not as possible outcome of X!

- It doesn't mean that, if we play several times, we'll always get an average of 3.94.


If we play the card game many times, our average winnings will be close to $3.94.   (And thus, it's probably foolish to pay $5 to play this game.) 



## Variance of a discrete distro

We know we won't always get the expected.  But, how far off will be?  What sort of variation will we see?  The measure is called Variance, it's just like standard deviation.

Formula:  Var[X] = sum((X-E(X))^2*P(X))

Idea:  We're finding average distance from the expected value, weighted by the probability it occurs.

### Single dice example

Recall the dice distro:

```{r}
X1 = c(1, 2, 3, 4, 5, 6)

P1 = c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6)

#recall:  E[x] = 3.5
#now, find variance:

sum((X1-3.5)^2*P1)

```

Discuss:

The variance for the dice distro is 2.917.  A more intuitive version is the standard deviation for the distro:

SD[x] = sqrt(Var[x])

```{r}
sqrt(2.916667)
```

Our std dev is 1.707.  This means:  our expected value for the dice distro is 3.5.  On average, dice rolls will fall within 1.707825 of this.

3.5 +- 1.707825

Kind of a like a confidence interval.



### Data Example:  mpg

**Alert**  Lab 3 #8!!!!

```{r}
mpg %>% head
```

Let's make a prob distro for cyl.  Just like your lab!

```{r}
#easy way to get probs/relative frequencies

table(mpg$cyl) %>% prop.table()
```

Cool!  Here's the distro.  But, what if we want expected value and variance?

```{r}
#name/store it:

table(mpg$cyl) %>% prop.table() -> P

#still need the x vals.  They're the names:

P %>% names %>% as.numeric -> X

# as.numeric needed to make numbers
# dont' forget to use -> to store/remember your variable

```

Let's find expected and variance:

```{r}
#expected value:  sum(X*P)

sum(X*P)

```

```{r}
#find variance:  sum( (X-E(x))^2 * P(X))

sum((X-5.88889)^2*P)

```

```{r}
sqrt(2.585945)
```

Summary: E(x) = 5.88889, Var(X) = 2.585945, STDEV(X) = 1.608087


Sophie's question:  In variance, all the units are squared becuase of ^2.  That's hard to visualize/imagine, so std dev is sometimes better.  Stdev has the same units.

Interpretation:  If we sample many cars, we expect an average number of cylinders close to 5.88889.  

We know that not all cars will have 5.8 cyl (that's impossible), but we can say something about the variability.  On average, a car will have within 1.608087 cylinders of the mean 5.888889.  Somewhere in the range:

5.88889 +- 1.608087.


### One more gambling example.

The game:  you draw a card from a 52-card deck at random.

If you get a red card, you win $5.  If you get a spade (they're all black), you win $10.  If you get the ace of clubs, you get $20.  Otherwise, nothing.

a)  Find the prob distro.

```{r}
X2 = c(5,10,20,0)
P2 = c(26/52, 13/52, 1/52, 12/52)

```


b)  Find the expected value and variance and std dev.

```{r}
#expected:

sum(X2*P2)

#var:

sum((X2-5.384516)^2*P2)

# std dev

sqrt(16.19822)
```


## Linear Combinations
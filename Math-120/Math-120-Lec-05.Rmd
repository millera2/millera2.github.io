---
title: "Math 120 Week 04"
author: "Prof Miller"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    code_folding: show
    theme: cerulean
    df_print: paged
  
  
---

```{r setup, include=FALSE}

#---------- RSTUDIO STARTER V 2.0  --------------#
#                    -Prepared with care by  AM ;D
                
                                                                          
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE)      
library(tidyverse)                
library(ggthemes)                    

theme_set(theme_tufte() +                                     
  theme(text=element_text(family="sans")))  

#------------------------------------------------#
```


# Monday











## Interpreing coeffients of lin models

Linear models have two coefficients: b0 and b1 (the intercept and the slope, respectively).  The coefficients tell us something important about the *relationship* between the x and y variables.

Recall:  the x variable is what me make our predictions *based upon*.  The y variable is what we make predictions *about*.  

Example:  iris.  Suppose we wish to predict petal width based upon petal legth with a linear model.  

x = petal length
y = petal width

```{r}
plot(iris$Petal.Length, iris$Petal.Width)
```

## Interpret slope

For any line, slope means "steepness".  CHange in y over change in x.  

For linear models, the slope measures how drastic the effect of x is on y.  Bigger slope means that x has a bigger effect on y. 

Specifically:  if [x variable] increases by 1 [x unit], then we predict/expect the [y variable] to change by [slope] [y units.]

## Example:  interpret iris slope


x = petal length
y = petal width

```{r}
r <- cor(iris$Petal.Length, iris$Petal.Width)
r
sy <- sd(iris$Petal.Width)
sy
sx <- sd(iris$Petal.Length)
sx

```
Slope:

```{r}
r*sy/sx
```

Interpretation:  for every additional cm of an iris's petal length, we expect/predict it's petal width to increase by 0.416  cm.  

## lm in r

It's easy to make linear models in R: lm()

Notation is a little different:  the ~ symbol means "is related to" or "is a function of".

y~x


```{r}
lm(iris$Petal.Width~iris$Petal.Length)
```

## Example: mpg data

Suppose we wish to predict hwy mpg based upon displ (engine size, in L).  

x = displ
y = hwy


```{r}
lm(mpg$hwy~mpg$displ)
```


If a car's engine increases in size by 1L, then we expect its highway fuel effiency to decrease by 3.531 mpg.

## Interpreting y-int

Generally, the y-int for any line is the y-value where x=0.

For a lin model, the intercept is our PREDICTION when the x variable is zero.  

## Example: iris

x = Petal length
y = petal width

```{r}
lm(iris$Petal.Width~iris$Petal.Length)
```

When an iris's petal length is 0cm, we predict its pedal width to be -0.361cm.  

Why so weird?

```{r}
plot(iris$Petal.Length, iris$Petal.Width)
```

Doesn't make sense, because it's impossible to have a petal length of 0cm!  This is called "extrapolation": making preditions for y based on x-values outside the range of observations.  Bad, don't do it!

There's no guarantee that our model works outside of our range of observations.  The y-int is often an extrapolation.  

Junk in, junk out.  

## Interpreting residuals

A residual is a measure of error

residual = observed - expected

e = y - yhat

We hope these are small!  Examining and interpreting residuals is an key component of analyzing a linear model.

## Residual plot

Always use your human eyeballs first!!  For residuals, make a residual plot.  They're x-y plots just like scatterplots, BUT:

x = original x data 
y = residuals

Easy in R:  the lm() command automatically computes and stores residuals.

## Example:  mpg dislp vs hwy

x = displ
y = hwy

```{r}
plot(mpg$displ, mpg$hwy)
```
Caution:  y first in lm

```{r}
mpgModel <- lm(mpg$hwy~mpg$displ)

#plot residuals

# x = mpg$displ
# y = mpgModel$residuals

plot(mpg$displ, mpgModel$residuals)
```

Most important observation:  is there a "pattern" or not?

If there *is* a pattern, ie if errors are predictable, it suggests we ought to be able to find a better model (quadratic, logistic, exponential, etc...).





# Tuesday 

## warm up 1

We a dataset taken from measurements of 50 men, and we wish to predict their weight based upon their height.

Summary stats:

mean height: 69.4"
mean weight: 179.2 lbs

sd height: 3.1"
sd weight: 9.4 lbs

Correl coff r = 0.821

Find the equation for the regression line.


1st, slope:
```{r}
0.821*9.4/3.1
```

2nd, y-int:

```{r}
179.2 - 2.489*69.4
```

yhat = 6.463+2.489x

## warm up 2

Interpret the coefficients of the model in part 1 above.

If a man's height increases by 1 in, then we expect/predict his weight to increase 2.489 lbs.

If a man is zero inches tall, we predict his weight to be 6.463lbs.  Extrapolation!  

## Warm-up 3

Based upon the above model, what would you predict the weight of a man who's 65" tall to be?

Plug in x = 65!

```{r}
6.463 + 2.489*65
```

## warm-up 4

Suppose Mike is 65" and weighs 181 lbs.  What was the residual for this observation?

resis = obs - exp
      = y - yhat
      = 181 - 168.248
      
```{r}
181 - 168.248
```
      
Ie we under-approximated by 12.752 lbs.  

## Interpreting Residuals

Resids tell us a lot about how good/bad  appropriate/inappropriate a linear model is.

Main tool:  resid plot

x = original x data
y = residuals

**Most important observation in resid plot: **  Is there a "pattern"?

If there is a pattern in resid plot, then our errors are predictable!  In that case, we ought to be able to do better!  Ie, pattern in resids means that linear model isn't the best.

Two main types of "patterns":

- Line/curve.  If so, we're systematically over/under approximating in certain ranges.
- Heteroskeastic resids have change magnitute over the range.  The lin model is failing on one side of the range.

**Good residuals**  Homoskedastic (i.e. even disprsion across the range), more dense toward y=0 (most are small)..

## Example:  good resids

Data:  mpg cty vs hwy

```{r}
plot(mpg$cty, mpg$hwy)
#resids
mpgModel <- lm(data=mpg, hwy~cty)
plot(mpg$cty, mpgModel$residuals)
```

## Example:  bad resids

Data:  mpg displ vs hwy

```{r}
plot(mpg$displ, mpg$hwy)
# resids

mpgModel2 <- lm(data=mpg, hwy~displ)
plot(mpg$displ, mpgModel2$residuals)
```
Curve pattern suggests that linear model isn't the best.  

## Outliers

Q:  if scatterplot has an outlier, what affect does that have on the  model?
A:  it depends!

- Sometimes, outliers strengthen r!!  I.e., there really isn't a relationship, but the outlier might make it seem like there is.
- Sometimes, outlier weaken r!  Ie they make it seem like there isn't a lin rel, when really there is.
- Sometimes, outliers have little/no effect on the linear model!

Because of this, probably best to run the model both ways:  with and without outliers.  Only your human eyeballs can tell if outliers are having an effect!





# Wednesday

## Last important quality of lin models:  R^2

The "coeffecient of determination", R^2, is our last important measurement of how good our model is.  

Btw, this really is (r)^2.  I.e., if you know that r = 0.781, then 

```{r}
#r^2

.781^2
```

R^2 tells us something special:  it's the percentage of variation in our (y) data that's because of (due to) the linear relationship between x and y.

## example:  mpg

x = displ (engine size)
y = hwy (highway mpg)

First, r:

```{r}
cor(mpg$displ, mpg$hwy)
```

Then, r^2 = 

```{r}
(-.76602)^2
```

Of all the variation in fuel efficiency (hwy mpg), about 58.7% of it is due to the linear relationship between engine size (displ) and fuel efficiency (hwy).


Of course engine size isn't the ONLY variable affecting mpg,but how much does it account for?  A:  about 58.7%

## Example:  height and weight

Last time, we predicted men's weights based upon their heights.

r = 0.821

Find r^2 and interpret.  What does it say about these men in the sample?

```{r}
.821^2
```

About 67.4% of the variation we see in weight is because of the relationship between height and weight.  

Of course, height doesn't totally determine weight.  But, it accounts for about 67.4% of the variation we see.  

## Summaries of lm

In R, you can get all the good info about a linear model with the summary() command.

Exampe:  cereal data

```{r}
cerealData <- read.csv("../Data/Cereal_Data.csv")

head(cerealData)
```

Using this data, let's make a model to predict calories based upon sugar.

1st:  look at it

```{r}
plot(cerealData$Sugars, cerealData$Calories)
```

Looks like a moderately strong pos lin rel.
```{r}
cor(cerealData$Sugars, cerealData$Calories, use="complete.obs")
```

Let's look at the model with lm()

```{r}
cerealModel <- lm(data=cerealData, Calories~Sugars)

cerealModel$residuals

summary(cerealModel)

```
Interpret coeffs:

slope:  For each additional gram of sugar, we'd expect the number of calories to increase by 2.5356 calories.  

y-int:  if a cereal has 0 g sugar , we expect it to have 89 calories per serving.  (maybe not extrapolation!)

Coeff of determination:

About 32.1%  of the variation in calorie content is due to (explained by) the relationship with sugar content.

Residuals:

```{r}

completeCereal <- na.omit(cerealData)

## had to take care of missing "NA" values.  
#You guys don't need to know this.

newModel <- lm(data=completeCereal, Calories~Sugars)

plot(completeCereal$Sugars, newModel$residuals)

```

Interpretations:  no linear/curve pattern, looks homoskedastic.  These are good residuals!

Seems like linear model is the best we can do.





































```{r}
lm(data=iris, Sepal.Width~Sepal.Length)
```


```{r}
plot(iris$Sepal.Length, iris$Sepal.Width)
# abline(int, slope)
abline(-0.6188, 3.41895)
```





```{r}
houseData <- read.csv("../Data/House-Sales.csv")

boxplot(houseData$price)

# use subset to get rid of outliers

# example: filter out all houses more than 2mil

normalHouses <- subset(houseData, price < 1000000)

nrow(normalHouses)
nrow(houseData)

boxplot(normalHouses$price)

plot(normalHouses$sqft_living, normalHouses$price, pch=20)

summary(houseData$price)

summary(houseData$sqft_living)

```






---
title: "Math 120 Week 04"
author: "Prof Miller"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    code_folding: show
    theme: cerulean
    df_print: paged
  
  
---

```{r setup, include=FALSE}

#---------- RSTUDIO STARTER V 2.0  --------------#
#                    -Prepared with care by  AM ;D
                
                                                                          
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE)      
library(tidyverse)                
library(ggthemes)                    

theme_set(theme_tufte() +                                     
  theme(text=element_text(family="sans")))  

#------------------------------------------------#
```





















## Interpreing coeffients of lin models

Linear models have two coefficients: b0 and b1 (the intercept and the slope, respectively).  The coefficients tell us something important about the *relationship* between the x and y variables.

Recall:  the x variable is what me make our predictions *based upon*.  The y variable is what we make predictions *about*.  

Example:  iris.  Suppose we wish to predict petal width based upon petal legth with a linear model.  

x = petal length
y = petal width

```{r}
plot(iris$Petal.Length, iris$Petal.Width)
```

## Interpret slope

For any line, slope means "steepness".  CHange in y over change in x.  

For linear models, the slope measures how drastic the effect of x is on y.  Bigger slope means that x has a bigger effect on y. 

Specifically:  if [x variable] increases by 1 [x unit], then we predict/expect the [y variable] to change by [slope] [y units.]

## Example:  interpret iris slope


x = petal length
y = petal width

```{r}
r <- cor(iris$Petal.Length, iris$Petal.Width)
r
sy <- sd(iris$Petal.Width)
sy
sx <- sd(iris$Petal.Length)
sx

```
Slope:

```{r}
r*sy/sx
```

Interpretation:  for every additional cm of an iris's petal length, we expect/predict it's petal width to increase by 0.416  cm.  

## lm in r

It's easy to make linear models in R: lm()

Notation is a little different:  the ~ symbol means "is related to" or "is a function of".

y~x

```{r}
lm(iris$Petal.Width~iris$Petal.Length)
```

## Example: mpg data

Suppose we wish to predict hwy mpg based upon displ (engine size, in L).  

x = displ
y = hwy

```{r}
lm(mpg$hwy~mpg$displ)
```

If a car's engine increases in size by 1L, then we expect its highway fuel effiency to decrease by 3.531 mpg.

## Interpreting y-int

Generally, the y-int for any line is the y-value where x=0.

For a lin model, the intercept is our PREDICTION when the x variable is zero.  

## Example: iris

x = Petal length
y = petal width

```{r}
lm(iris$Petal.Width~iris$Petal.Length)
```

When an iris's petal length is 0cm, we predict its pedal width to be -0.361cm.  

Why so weird?

```{r}
plot(iris$Petal.Length, iris$Petal.Width)
```

Doesn't make sense, because it's impossible to have a petal length of 0cm!  This is called "extrapolation": making preditions for y based on x-values outside the range of observations.  Bad, don't do it!

There's no guarantee that our model works outside of our range of observations.  The y-int is often an extrapolation.  

Junk in, junk out.  

## Interpreting residuals

A residual is a measure of error

residual = observed - expected

e = y - yhat

We hope these are small!  Examining and interpreting residuals is an key component of analyzing a linear model.

## Residual plot

Always use your human eyeballs first!!  For residuals, make a residual plot.  They're x-y plots just like scatterplots, BUT:

x = original x data 
y = residuals

Easy in R:  the lm() command automatically computes and stores residuals.

## Example:  mpg dislp vs hwy

x = displ
y = hwy

```{r}
plot(mpg$displ, mpg$hwy)
```
Caution:  y first in lm
```{r}
mpgModel <- lm(mpg$hwy~mpg$displ)

#plot residuals

# x = mpg$displ
# y = mpgModel$residuals

plot(mpg$displ, mpgModel$residuals)
```

Most important observation:  is there a "pattern" or not?

If there *is* a pattern, ie if errors are predictable, it suggests we ought to be able to find a better model (quadratic, logistic, exponential, etc...).


-----------------------------------
----------------------------




--------------------




# Tuesay

## Warm-up example 1 (finding equation)

Suppose we have data on 50 men, and we wish to predict their weight based upon their height.  

We find:

mean height = 69.4"
mean weight = 174 lbs

sd height = 3.2"
sd weight = 8.7 lbs

r = 0.86

Find the equation for the least squares line.  

First:  slope.   

```{r}
b1 <- .86*8.7/3.2
b1
```

Next:  int

```{r}
174 - 2.338*69.4
```

Full equation:  yhat = 11.743 + 2.338x


## Warm-up 2  

Based upon the above model, predict the weight for a man who is 65".  

Plug in x=65

```{r}
11.743 + 2.338*65
```

## Warm-up 3

Interpret the the coefficients of the model above.

Slope:  For each additional inch taller a man is, we expect his weight to increase by 2.338 lbs.  

Int:  If  a man is 0 in tall, we expect him to weigh 11.743 lbs.  Extrapolation!  


## Warm-up 4

If the man in part 3 above actually weighed 181 lbs, what's the residual for his measurment?

Resid = obs - expected
      =  y - yhat
      
```{r}
181 - 163.713
```
      
Ie our model under-approximated his weight by 17.287 lbs.  

## Interpeting residual plots

Recall, analyzing residuals is an important tool in evaulating how good a linear model is.

Recall, a residual plot has 

x = original x data
y = residuals

```{r}
irisModel <- lm(data=iris, Petal.Width~Petal.Length)

plot(iris$Petal.Length, irisModel$residuals)

```

** Most important thing **:  Do you see a "pattern"?  

If there is a pattern, it suggests that our errors are predictable!  We ought to be able to predict them, i.e., there ought to be a better model than linear.  Linear model not the best!

If no pattern, then lin model is probably as good as it gets.

Good resid:  most are close to zero, even dispersion on both sides, no form or pattern.  Blob!  Densest at y=0.  Called:  homoskedastic.  

Q:  What kinds of "patterns" might we see?  

A:  Two main kinds

1) Curve/line/polynomial

Example:  mpg

```{r}
plot(mpg$displ, mpg$hwy)
cor(mpg$displ, mpg$hwy)

```

Note: looks like curvy pattern.  let's look at resids

```{r}
mpgModel <- lm(data=mpg, hwy~displ)
#plot resids
plot(mpg$displ, mpgModel$residuals)
```

Woah!  Even though r is rather strong (-.76), looks like there might be a better model.  

2)  Heteroskedasticity.  Variance not even across the range.  Bad!  Lin model is failing in places.



## Outliers and Influential Values

Outliers can strongly affect our model.  But, sometimes they have little effect!

- Sometimes, an outlier can strongly increase correl coeff r!  I.e., even though there really isn't a lin rel, the outlier might make it seem like there is!  

- Sometimes, an outlier can strongly decrease r.  I.e., even though there's a strong relationship, the outlier might make it seem otherwise!

- Sometimes, outliers have little effect on the model!

See notes for examples.

Because of this, it's *vital* to look at the scatterplot.  Ie the coeff r alone is not enough!  No way to know how the outlier affects the scatterplot unless you look at it.

Since it's easy in software to run scatterplot, probably best to do both:  with and without outlier.  Did it have an effect?  Check!






```{r}
sd(mpg$displ)

cor(mpg$displ, mpg$hwy)
```












-----------------------------

-----------------------


------=-----





# Tuesday 

## warm up 1

We a dataset taken from measurements of 50 men, and we wish to predict their weight based upon their height.

Summary stats:

mean height: 69.4"
mean weight: 179.2 lbs

sd height: 3.1"
sd weight: 9.4 lbs

Correl coff r = 0.821

Find the equation for the regression line.


1st, slope:
```{r}
0.821*9.4/3.1
```

2nd, y-int:

```{r}
179.2 - 2.489*69.4
```

yhat = 6.463+2.489x

## warm up 2

Interpret the coefficients of the model in part 1 above.

If a man's height increases by 1 in, then we expect/predict his weight to increase 2.489 lbs.

If a man is zero inches tall, we predict his weight to be 6.463lbs.  Extrapolation!  

## Warm-up 3

Based upon the above model, what would you predict the weight of a man who's 65" tall to be?

Plug in x = 65!

```{r}
6.463 + 2.489*65
```

## warm-up 4

Suppose Mike is 65" and weighs 181 lbs.  What was the residual for this observation?

resis = obs - exp
      = y - yhat
      = 181 - 168.248
      
```{r}
181 - 168.248
```
      
Ie we under-approximated by 12.752 lbs.  

## Interpreting Residuals

Resids tell us a lot about how good/bad  appropriate/inappropriate a linear model is.

Main tool:  resid plot

x = original x data
y = residuals

**Most important observation in resid plot: **  Is there a "pattern"?

If there is a pattern in resid plot, then our errors are predictable!  In that case, we ought to be able to do better!  Ie, pattern in resids means that linear model isn't the best.

Two main types of "patterns":

- Line/curve.  If so, we're systematically over/under approximating in certain ranges.
- Heteroskeastic resids have change magnitute over the range.  The lin model is failing on one side of the range.

**Good residuals**  Homoskedastic (i.e. even disprsion across the range), more dense toward y=0 (most are small)..

## Example:  good resids

Data:  mpg cty vs hwy

```{r}
plot(mpg$cty, mpg$hwy)
#resids
mpgModel <- lm(data=mpg, hwy~cty)
plot(mpg$cty, mpgModel$residuals)
```

## Example:  bad resids

Data:  mpg displ vs hwy

```{r}
plot(mpg$displ, mpg$hwy)
# resids

mpgModel2 <- lm(data=mpg, hwy~displ)
plot(mpg$displ, mpgModel2$residuals)
```
Curve pattern suggests that linear model isn't the best.  

## Outliers

Q:  if scatterplot has an outlier, what affect does that have on the  model?
A:  it depends!

- Sometimes, outliers strengthen r!!  I.e., there really isn't a relationship, but the outlier might make it seem like there is.
- Sometimes, outlier weaken r!  Ie they make it seem like there isn't a lin rel, when really there is.
- Sometimes, outliers have little/no effect on the linear model!

Because of this, probably best to run the model both ways:  with and without outliers.  Only your human eyeballs can tell if outliers are having an effect!














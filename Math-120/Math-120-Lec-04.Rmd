---
title: "Math 120 Week 04"
author: "Prof Miller"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    code_folding: show
    theme: cerulean
    df_print: paged
  
  
---

```{r setup, include=FALSE}

#---------- RSTUDIO STARTER V 2.0  --------------#
#                    -Prepared with care by  AM ;D
                
                                                                          
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE)      
library(tidyverse)                
library(ggthemes)                    

theme_set(theme_tufte() +                                     
  theme(text=element_text(family="sans")))  

#------------------------------------------------#
```





















## Interpreing coeffients of lin models

Linear models have two coefficients: b0 and b1 (the intercept and the slope, respectively).  The coefficients tell us something important about the *relationship* between the x and y variables.

Recall:  the x variable is what me make our predictions *based upon*.  The y variable is what we make predictions *about*.  

Example:  iris.  Suppose we wish to predict petal width based upon petal legth with a linear model.  

x = petal length
y = petal width

```{r}
plot(iris$Petal.Length, iris$Petal.Width)
```

## Interpret slope

For any line, slope means "steepness".  CHange in y over change in x.  

For linear models, the slope measures how drastic the effect of x is on y.  Bigger slope means that x has a bigger effect on y. 

Specifically:  if [x variable] increases by 1 [x unit], then we predict/expect the [y variable] to change by [slope] [y units.]

## Example:  interpret iris slope


x = petal length
y = petal width

```{r}
r <- cor(iris$Petal.Length, iris$Petal.Width)
r
sy <- sd(iris$Petal.Width)
sy
sx <- sd(iris$Petal.Length)
sx

```
Slope:

```{r}
r*sy/sx
```

Interpretation:  for every additional cm of an iris's petal length, we expect/predict it's petal width to increase by 0.416  cm.  

## lm in r

It's easy to make linear models in R: lm()

Notation is a little different:  the ~ symbol means "is related to" or "is a function of".

y~x

```{r}
lm(iris$Petal.Width~iris$Petal.Length)
```

## Example: mpg data

Suppose we wish to predict hwy mpg based upon displ (engine size, in L).  

x = displ
y = hwy

```{r}
lm(mpg$hwy~mpg$displ)
```

If a car's engine increases in size by 1L, then we expect its highway fuel effiency to decrease by 3.531 mpg.

## Interpreting y-int

Generally, the y-int for any line is the y-value where x=0.

For a lin model, the intercept is our PREDICTION when the x variable is zero.  

## Example: iris

x = Petal length
y = petal width

```{r}
lm(iris$Petal.Width~iris$Petal.Length)
```

When an iris's petal length is 0cm, we predict its pedal width to be -0.361cm.  

Why so weird?

```{r}
plot(iris$Petal.Length, iris$Petal.Width)
```

Doesn't make sense, because it's impossible to have a petal length of 0cm!  This is called "extrapolation": making preditions for y based on x-values outside the range of observations.  Bad, don't do it!

There's no guarantee that our model works outside of our range of observations.  The y-int is often an extrapolation.  

Junk in, junk out.  

## Interpreting residuals

A residual is a measure of error

residual = observed - expected

e = y - yhat

We hope these are small!  Examining and interpreting residuals is an key component of analyzing a linear model.

## Residual plot

Always use your human eyeballs first!!  For residuals, make a residual plot.  They're x-y plots just like scatterplots, BUT:

x = original x data 
y = residuals

Easy in R:  the lm() command automatically computes and stores residuals.

## Example:  mpg dislp vs hwy

x = displ
y = hwy

```{r}
plot(mpg$displ, mpg$hwy)
```
Caution:  y first in lm
```{r}
mpgModel <- lm(mpg$hwy~mpg$displ)

#plot residuals

# x = mpg$displ
# y = mpgModel$residuals

plot(mpg$displ, mpgModel$residuals)
```

Most important observation:  is there a "pattern" or not?

If there *is* a pattern, ie if errors are predictable, it suggests we ought to be able to find a better model (quadratic, logistic, exponential, etc...).










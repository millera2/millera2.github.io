---
title: "DA 101 Lec 13"
author: "Your Name"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    code_folding: show
    theme: simplex
    df_print: paged
  
---

```{r setup, include=FALSE}

#---------- RSTUDIO STARTER V 2.0  --------------#
#                    -Prepared with care by  AM ;D
                
                                                                          
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE)      
library(tidyverse)                
library(ggthemes)                    

theme_set(theme_tufte() +                                     
  theme(text=element_text(family="sans")))  

#------------------------------------------------#
```


## Linear Regression


Warm-up example:  mpg dataset, x=displ (engine size in L), y=cty (fuel efficiency in city conditions).

First thing always:  take a look!

```{r}


mpg %>% ggplot(aes(x=displ, y=cty))+geom_point()


```

Makes sense:  as engines get larger, they get less efficient.

Looks fairly strong to my human eyeballs.  But, we need an objective measurement.  It' r, the correlatin coefficient:

```{r}
cor(mpg$displ, mpg$cty)
```

Makes sense, we saw a moderately strong negative linear relationshp.  How can we take it further?

Important question:  what exactly is the "model"?  The straight line that comes as close as possible to the points.

```{r}

mpg %>% ggplot(aes(displ, cty))+
  geom_point()+
  geom_smooth(method = "lm")

```

Important things about the model:

- What makes it the "best"?  The best fit line/lin reg line/the line minimizes the square errors.  
  - Here, errors are the vertical distance between the line and the points.  Solving this problem requires calculus and linear algebra, but it's easy in R. The best fit line is the one whose square residuals:
  
  (y-yhat)^2
  
are as small as possible.

- The model itslef is just a line.  In high school you learned:

   y = mx + b
   
- Works the same here, but we use different symbols in this context:

  yhat = b_0 + b_1*x
  
- I.e., the slope is b_1, the intercept is b_0.  Same thing, different letters.  To know the model, we need to know these "coefficients".  

Fortunately, doing this in R is easy:

```{r}
#make a "linear model"

#think:  y is a function of x
#in R, write:  y ~ x
# the ~ means "is a function of"

# for us, y = mpg$cty,  x=mpg$displ

lm( mpg$cty ~ mpg$displ)


```

Upon runnning the lm, we immediately get the coefficints.

Here, our equation fot the line is:

yhat = 25.99 - 2.63x

Note that the lm object has **lots** of useful info.  We shoud name to save for later with <-

The lm has lots of built-in variables that you can access with $

```{r}
mpgModel <- lm(mpg$cty~mpg$displ)

mpgModel$coefficients

```

Very important feature:  model summary!

```{r}
summary(mpgModel)
```

We'll eventually know what all this stuff means!  Right, now, notice especially these three things:

- Slope  -2.63
- Intercept  25.9115
- R^2 = .6376

Btw, r^2 really is (r) ^2.  If I want to find the correlation coefficient, I can sqrt(r^2).  Here:

```{r}
sqrt(.6376)
```

Making these models and observing their featuers is the easy part.  The tough part is interpreting them.  What do these fancy numbers actually mean?  Here, what have we learned about these cars?

### Interpreting coefficients, b_0 and b_1

Every line has a slope and intercept, but they mean something special in this case:

Slope:  visually, it shows steepness.  Here in lin reg, it shows how drastic or severe the relatinship is.  In particular:  if x increase by 1 unit, we expect y to change by [slope].  

Here:  If a car's engine increases in size by 1L, then we expect its city fuel efficiency to decrease by 2.63mpg.

Intercept:  in general, the y-int is what y should be when x is zero.

Here:  If a car's engine is 0L, then we expect its fuel efficiency to be 25.91mpg.

Note:  as frequently happens, the intercept isn't very useful or sensible since there aren't any cars with an engine size of zero!  Doesn't make sense!  This is an example of **extrapolation**, i.e., plugging in x-values that are unuasual or unreasonable.

Remember:  Junk in -> Junk out.  Our model only works for predictions on "reasonable" values.

For this reason, the slope is the most important coefficient.  

### Interpreting R^2

It's called the "coefficient of determination", R^2.  It's literally (r)^2.  It has a very, very special meaning/interpretation:

- R^2 is the percentage of variation in our y data that's attributatable to/because of the linear relationship between x and y.

In this example:  it makes sense that bigger engines are less efficient.  BUT, certainly engine size isn't the -only- factor influencing mpg.  Question:  how much *does* the engine size count for.

Here, r^2 = .6376.  That means about 64% of the varation in fuel efficiency is due to the influence of displacement.

In generaly, we know that no single variable can explain everyting (unless r=1, which never happens).  R^2 tells us how "important" our variable is; how much of the change in our dependent variable is because of x?

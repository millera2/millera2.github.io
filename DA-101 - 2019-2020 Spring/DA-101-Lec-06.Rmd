---
title: "DA 101 Week 6"
author: "Prof Miller"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    code_folding: show
    theme: cerulean
    df_print: paged
  
---

```{r setup, include=FALSE}

#---------- RSTUDIO STARTER V 2.0  --------------#
#                    -Prepared with care by  AM ;D
                
                                                                          
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE)      
library(tidyverse)                
library(ggthemes)                    

theme_set(theme_tufte() +                                     
  theme(text=element_text(family="sans")))  

#------------------------------------------------#
```



# Monday

## Hypothesis Tests

Remember, all have 4 parts:

- Hypotheses:

  - Null hypothesis:  H0.  The assumption we make.
  - Alt hypotheses:  Ha.  The thing we're trying to prove.  
  
- Test stat.  Summary of our actual data.  I.e., what we observed!

- p-value.  Most important part!  A p val is the probability of observing a sample result that's as (or more) extreme than the one we got, assuming the H0 is true. P-vals work because of the central limit theorem:  as long as sample size is big, sample stats like mean and prortion follow a normal dist.

NOTE:  small p-vals provide evidence AGAINST H0.  If p-val is small, then our sample data is unlikely according to H0.  We trust our data, and reject H0!

- Conclusion.  If p-val is "small enough", we reject H0 and find evidence to support Ha.  Yay!  This is what we hoped for!

Q:  How small is "small enough"?  The cut-off is called the "significance level".  Symbol:  alpha.  Most common:  alpha = .05.  I.e., if our test stat is smaller than .05, we reject H0.  Otherwise, if p > .05, we fail to reject.

NOTE:  the choice of alpha is totally arbitrary!  It's up to you!  alpha = .05 is common, but not divine or necessary.  

NOTE:  In labs, always use alpha = .05.  But, DO mention that you're doing this.  

## Statstical Inference - stats vs parameters

Statistical inference is the process of making conclusions about a population based upon sample data.

THE REAL PROBLEM:  we only ever have sample data!!  BUT, we want to conclusions about populations!  Even the best samples are sometimes unrepresentative.

Sample data is just a slice of the real thing.  Statistical inference is the process of making mathemtically-supported conclusions based on available info.  

Every quality of data has two versions:

A **statistic** is a summary of sample data.  Exmamples:  

- sample mean = xbar
- sample std dev = s_x
- sample proportion = phat

A **parameter** is a summary of the whole population.  Examples:

- pop mean = mu
- pop std dev = sigma
- pop proportion = p

We only every have the stats.  But we hope to say something smart about these parameters!

## 2-t tests

2-t tests are used to answer the following question:  is there a difference in mean between two groups?

Parameter:  mu1 - mu2

## Example:  mpg dataset

Is there evidence of a difference in cty fuel efficiency between pickups and suvs?

1)  Hypotheses

H0:  mu1 - mu2 = 0 (I.e., no difference in mpg)
Ha:  mu1 - mu2 != 0 (I.e., there's a difference)

2) Tests stat.

First, filter into two groups.

```{r}
head(mpg)
unique(mpg$class)

pickups <- filter(mpg, class == "pickup")

suvs <- filter(mpg, class == "suv")

t.test(pickups$cty, suvs$cty)

```

Actual difference:  xbar1 - xbar2 = -0.5

Q:  is that difference big enough to conclude that the populations are different?

Test sta:  t = -1.0626

3) p-value = 0.2914.  Here:  IF it's true that there's no difference in mean mpg between suvs and pickups, then the probability of observing a difference in sample mean that's -.5 mpg (or larger) is 29.14%.

4)  Since our p-value is larger than .05, we fail to reject H0.  Our sample data seems to agree with the null hypothesis.  We didn't find evidence of a difference at the population level.  This result is NOT statistically significant.

## Confidence Intervals

In a hyp test, you start with a question about a parameter and answer with sample data.

In a CI, we start with our sample result, and make an estimate about the population parameter.

Both are shown in the t.test() command:

```{r}
t.test(pickups$cty, suvs$cty)

```

In the mpg example, the CI means this:  using the CLT, we are 95% confidewnc that the true differene in mean (mu1-mu2) is between -1.437 and 0.437.

NOTE:  hyp tests and CIs will ALWAYS give you the same conclusion about the null hypothesis.  If the H0 value is inside the CI, then we fail to reject.

```{r}
unique(mpg$class)
```

## Example

Using only the 95% CI, decide if there's evidence of a differene in hwy mpg between compacts and 2seaters.


```{r}
compacts <- filter(mpg, class=="compact")
twoSeaters <- filter(mpg, class=="2seater")

t.test(compacts$hwy, twoSeaters$hwy)

```

We are 95% confident that the true difference in mpg between compacts and 2seaters is between 1.77mpg and 5.23mpg.  We have strong evidence of a differencein hwy fuel efficiencty between the two groups.






# Wednesday

## Errors in hypothesis testing.

Idea:  these tools fail sometimes!

Hyp tests can get it wrong!  Confidence intervals can fail!  What then?

Every hyp test ends in one of two ways:

- p < alpha, we reject H0.  This gives evidence to support Ha. (alpha = .05 ususally)
- p > alpha, we fail to reject H0.  We didnt' find evidence to support Ha. Bummer!  THINK:  The Ha is our claim that we're trying to support!

Of course, since these inference tools are based on samples, and sample vary randomly, sometimes our conclusions will be incorrect!

- Type I error:  we reject the H0, but we shouldn't have because H0 is TRUE!
- Type II error:  we fail to reject the H0, but we should have because H0 is FALSE!

Also called:  false positive, false negative respectively.  

We'll never know for sure, BUT we can know something about the likelihood of these errors!

Also: consider the consequences!

## Example 1:  Water Quality

An EPA enforcement office wonders if the local water supply contains too much of Toxin A.  The safe level is 0.8 g/ml.  

1) What are the hypotheses of the test she'll do?

H0:  mean level = 0.8
Ha:  mean level > 0.8


2) What does Type 1 error mean in this case?

We think that the water is dangers, when actually it's safe.

3) What does Type 2 error mean in this case?

We failed to reject H0.  There's not strong evidence that mean level > 0.8.  We conclude that the water is safe, when actually it's dangerous.

If we make type I, we might waste money cleaning safe water.  
If we make type II, then people would drink dangerous water!

Seems like Type II is worse!

## Example 2

Think of a hyp test as a criminal trial:

H0:  Defendent is innocent
Ha:  Defendent is guilty

1)  What does type I error mean?

We think they're guilty, but actually they're innocent!

2)  What does type II error mean?

Fail to reject H0, no strong evidence of guilt.  We conclude they're innocent.  Acutally, they're guilty!

3) Which do you think is worse?

Here, seems like type I is worse.  Don't want to punish innocent people!  (Room for debate). 


## Moral of the story:  It Depends!

It's not always the case that Type I is worse or Type II is worse.  

We can't eliminate the chance of error.  BUT, we can tip the scales to make Type I or Type II more likely than the other.

## alpha, beta, and power

We know something about how likely these errors are.

1) The probabilty of a type I error is alpha.  THE alpha!  Two important facts:
 - Usually, there's a 5% chance of type I error!
 - Since alpha is arbitrary, YOU can choose the prob of type I error!
 
Ie, if a type I error is really, really bad, then we could use a smaller alpha!  I.e., alpha = .01.  

THINK:  In a criminal trial, we say there can't be a "shadow of a doubt".  

2) The probability of type II error is beta.  Unfortunately, we can never directly compute beta.  BECAUSE, if type II, H0 is false, and we DON'T know the true population parameter (mu, mu1-mu2, etc). Thus, can't do probability computations.

HOWEVER, alpha and beta have a predictable relationship:  they're inverses.

- If alpha increases, then beta decreases
- If alpha decreases, then beta increases

So, if a type II error is really really bad, then we choose alpha to be larger.

To keep our test "honest", it's critical to decide on alpha BEFORE conducting the test.  
"p hacking":  Fiddling with the test to get a significant result.  

Ex: Conduct the test, get p = 0.055, decide to use alpha =.06.  

3) Power.  Power is the probability of rejecting H0 when H0 is false.  I.e., success!  This what we hope for!

Power and beta have an important relationship:

  power = 1 - beta
  beta = 1 - power
  
They're compliments!

Q: if alpha increases, what happens to power?  

A:  Bigger alpha means we reject more.  Power is when we reject, thus bigger alpha means bigger power!

SO, in some sense, big alpha is good!  But it comes at a price:  greater chance of false positive!

Or, power = 1-beta.  If alpha gets bigger, beta smaller, thus 1-beta gets bigger.  

## Important factors affecting all three

1)  Sample size.  Generally, sample size is "good".  Does everything you hope for:
- bigger n decreases *both* alpha and beta.  It increases power.

2) The distance between the null hyp value and the "true" population parameter.  If this gap is bigger, the test has more power.






# Monday

## Correlation and Linear Regression

We use the technique to investigate relationships between two quantitative variables (later, multiple quantitative variables).  

You know the visual:  scatterplot.

## Example:  mpg data

```{r}
mpg %>% ggplot(aes(x=cty, y=hwy))+geom_point()
```

Observations:  clearly a relationship.  In particular:

- positive (x and y get bigger together)
- linear (question:  equation for the line?)
- fairly strong

Looks strong, but we need an objective measurement:  the correlation coefficient, r.

NOT TO BE CONFUSED WITH THE NAME OF OUR SOFTWARE!


## Finding the correlation coefficient

```{r}
cor(mpg$cty, mpg$hwy)
```

r = 0.956

Cool, but what does this mean?  Two things: 

- strength.  
  - closer to +1 means strong positive relationship
  - closer to -1 means strong negative relationship
  - close to zero means no/weak *linear* relationship
  
Note:  always true -1 <= r <= 1

WARNING:  r only measures LINEAR relationships.  Could be other kinds!

## Examples:  r close to zero

```{r}
iris %>% ggplot(aes(x=Sepal.Length, y = Sepal.Width))+geom_point()
```

```{r}
cor(iris$Sepal.Length, iris$Sepal.Width)
```

## Example: mpg

x = displacement (engine size)
y = hwy mpg

```{r}
mpg %>% ggplot(aes(x=displ, y=hwy))+geom_point()
```

Seems, as expected, like a negative relationship.  Mod strong.  R?

```{r}
cor(mpg$displ, mpg$hwy)
```


## Least-squares line

Zillion names:

- least-squares line
- regression line
- best fit line
- trend line

Idea:  it's the (unique) line thet minimizes the errors in prediction.  

NOTE: these "errors" are called "residuals", i.e. the vertical distance between the points and line.

residuals = observed         -   expercted
          = y-coord of point -   y coord on the line
          
## Finding equation of best-fit line

Only need two things:  slope and intercept

HS:  y = mx + b

Here:  yhat = b0 + b1x

Where:

- b0 = intercept
- b1 = slope

In R:  use `lm()` command (short for "linear model")

## Example:  mpg

Let's find the equation of the best-fit line where:

x = cty mpg
y = hwy mpg

In lm, write:  

y ~ x

~ means "is a function of"

```{r}
lm(mpg$hwy ~ mpg$cty)

# or

lm( data = mpg, hwy~cty)
```

Final equation:  yhat = 0.892 + 1.337*x

This equation predicts hwy mpg based upon cty mpg.

Example:  predict the hwy mpg of a car with cty mpg of 25.

I.e:  x = 25.  Easy:  plug in x!

```{r}
0.892 + 1.337*25
```

I'd expect the car's hwy mpg to be 34.317mpg.  

## Plotting the line

In ggplot:  + geom_smooth(method = "lm")

```{r}
mpg %>% ggplot(aes(x=cty, y=hwy))+geom_point()+geom_smooth(method = "lm")
```

## Example: displ and hwy


Make a linear model to predict the hwy mpg of a car with engine displ of 3.2L.


NOTE:  the y variable is what we make predictions ABOUT.  I.e., DEPENDENT VARIABLE.  Here: hwy

the x variable is what we're basing our predictions on.  I.e., the INDEPENDENT variable.  Here:  displ

```{r}
lm(data=mpg, hwy~displ)
```

Our line:  yhat = 35.698 - 3.531x

x = displ, yhat is our prediction about hwy mpg.

If x = 3.2,

```{r}
35.698 - 3.531*3.2
```

## Interpreting the coefficients of the model

Linear models have two "coefficients", the slope and intercept.

In general, slope tells us how "steep" a relationship is.  I.e., it's how much y changes as a reasult of a 1 unit change in x.  

In a linear model, if the x variable increases by 1 unite, then we expect/predict the y variable to change (inc/dec) by the slope (y units).  

## Example:  cty mpg and hwy mpg

x = cty
y = hwy

```{r}
lm(data=mpg, hwy~cty)
```

Slope = 1.337.  If the cty mpg of a car increases by 1 mpg, then we predict its hwy mpg increases by 1.337 mpg.

## Example:

x = displ (liters)
y = hwy   (mpg)

Slope: -3.531










# Tuesday

## Warm-up

Make a linear model to predict Petal width based upon Sepal width in the iris dataset.

1)  Find the equation of the line
2)  Interpret the coefficients

```{r}
head(iris)

iris %>% ggplot(aes(x=Sepal.Width, y=Petal.Width))+geom_point()+geom_smooth(method = "lm")
```

y~x

```{r}
lm(data=iris, Petal.Width~Sepal.Width)
```

yhat = 3.1569 - 0.6403x

Intercept:  If a plant has sepal width of 0cm, then we predict it's petal width to be 3.1569 cm.  Note, this is an example of "extrapolation", i.e., making a prediction based on an x value that's well outside the range of observed values.  Since x=0 doesn't make sense here, we should be skeptical of the prediction.  There's no reason to suspect that the model holds outside the range of observations.

Q:  does it make sense to predict the petal width for a plant with sepal width of 15cm?  No, also extrapolation.

Extrapolation:  junk in, junk out!

Slope:  If an iris sepal width increases by 1cm, then we predict it's petal width to decrease by 0.6403cm.  


## Evaluating Models

Are they any good?

Key observation:  residuals.

Generally, we hope residuals are small.   Remember the least squares line is the unique line that minimizes square residuals.  

We always want to anaylyze residuals, and the first thing:  make a plot!  Use your human eyeballs!

Example:  mpg data.  Predict hwy mpg based on displ (engine size in L)

```{r}
mpgModel <- lm(data = mpg, hwy~displ)
```

Note:  the lm command actually creates an object with many variables!  Always store it!

First thing:  summary

```{r}
summary(mpgModel)
```

ALso, lm stores lots of variables

```{r}
summary(mpgModel$residuals)
```

Visual:  a residual plot is an x-y plot (like a scatterplot) BUT:

x = original data (just like scatterplot)
y = residuals (different from scatterplot!)

Note:  since these variables are stored in different tables, it's easier to use "qplot()"

Qplot is a "simple" version of ggplot.  Still has all cool features of ggplot.


Original:

```{r}
qplot(mpg$displ, mpg$hwy)
```

```{r}
# resid plot
# x = displ
# y = hwy

qplot(x=mpg$displ, y=mpgModel$residuals)

```

## What to look for in residuals

First question:  do we see a visual "pattern" in residuals?

Idea:  residuals are error. If there's a pattern in residuals, then the errors are predictable! But then, we ought to be able to use a better model!!  We ought to be able to "fix" predicable errors!

If residual pattern, linear model is not the best!
If no discernable pattern, linear model is as good as it gets.

"Good resids" are just a big mess.

Q:  What's a "pattern"?

A:  Two common ones:

1)  "curve" pattern
2) "heteroskedastic" residuals.  Magnitude increases on one side.


## Good resids

Hope that our residuals are "homoskedastic"  (i.e., even spread/magnitude across the range) and that the reduals are roughtly normally distributed (i.e, most are close to zero, few are large magnitude).

To check homoskedasticity, use resid plot.

To check normal dist, use a histogram.

```{r}
qplot(mpgModel$residuals, geom="histogram", bins=12)
```

The resids are "ok", somewhat normal dist.  BUT, there's some right skew, i.e, some rather large resids in the model.  Not idea.

## Coeffifient of determination

The coeff of det, R^2, is also a key measure of how "good" a model is.

Note:  this really is (r)^2, where r = correlation coefficient.

```{r}
cor(mpg$displ, mpg$hwy)
```

```{r}
(-.76602)^2
```

```{r}
summary(mpgModel)
```


What does R^2 mean?  This:  R^2 is the percent of variation in the y data that's because/attribuatle to the linear relationship between x and y.


We know x can't be the only factor for y.  But, how much does it account for?  R^2!

For mpgModel:

x = displ (engine size)
y = hwy mpg

R^2 = .5868

Of all the variation in fuel efficiency, about 58.68% is due to effect of the relationship between engine size and hwy mpg.














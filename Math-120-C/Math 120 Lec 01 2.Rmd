---
title: "Math 120 Lec 01"
author: "Prof Miller"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    code_folding: show
    theme: yeti
    df_print: paged
  
  
---

```{r setup, include=FALSE}

#---------- RSTUDIO STARTER V 2.0  --------------#
#                    -Prepared with care by  AM ;D
                
                                                                          
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE)      
library(tidyverse)                
library(ggthemes)                    

theme_set(theme_tufte() +                                     
  theme(text=element_text(family="sans")))  

#------------------------------------------------#
```



# Week 1

## Wednesday Feb 3


This is Rstudio!

I can write stuff, but I can also do math:

```{r}
1+1
```

Sometimes you'll see me use code, like this:  let's look at the famous "iris" dataset. 

```{r}
head(iris)
```


You don't need to know any R code, but you ABSOLUTELY SHOULD understand all mathematical computations.



## 1.2: Data Basics

What's a dataset?

There are a several ways to represent data.  In 120, we'll always use **rectangular data**.

Data is rectangular if:

- Each row is an individual (case)
- Each column is a variable, i.e., a quality of the individuals that we're measuring
- Each cell/entry contins a specivic observation/value

Goal:  look at a dataset, and say smart things about the individuals in it.

### Types of Data

Not all data is created equal!

Need to be **very** careful -- different data types require different tools to analyze.


First major division:  some data are numbers, some are not!

Ex:  makes sense to find mean sepal length, but makes no sense to find "mean species".

Broadly speaking:

-quantitative/numerical data:  it's a number
-qualitative/categorical data:  it's not a number

We can subdivide further:

Quantitative data can be either:

- discrete: countable.  whole numbers.  Ex:  number of students in this class.  # of siblings you have.  Etc.  
- continuous:  measurable.  could take any real number (within reasonable bounds).  Ex:  height, GPA, temperature, weight.  In general:  any physical measurement of the universe.  

Caveats:

- Height.  We usually round our height to whole number.  But it's still cts!  Depends on the nature of the variable, not how we round it.  

- Money.  Let's just agree that money is continuous.



Qualitative data can also be subdivided into two types:

- nominal data: no intrinsic order.  Ex: color,  species, gender, nationality, language, 
- ordinal data: DO have intrinsic order.  Ex:  Age, Month, Alphabet, Income level, Educational attainment,  minutes in the our, year in school, seasons

Caveats:

- Age:  might seem like quantitative, but careful, it's a category!  Same with minutes in the hour.  

- We can always ASSIGN and ordering.  Like color:  we -could- order by wavelength.  


### Metadata

Metadata is data about data.

Almost always, we study data collected by someone else.  Not always clear what the variables represent.  Ex:  "mpg dataset"

```{r}
head(mpg)
```

Metadata tells:

- what the cases are (rows)
- what variables mean (columns)
- what the units are (when applicable)


## Describing Data - Visualizations

Always, our first tool for describing data is a **visualization**.  A picture is worth a 1000 words.

There are many different types of visualaztions.  We need to be carefu to choose the right one for our data.

### Visualizations for a single quantitative variable

The most important visualization for a single quant variable is a **histogram**.  

Whenver we constrtuct a histo, there are three important qualities to observe:

- Center.  What's a common value in the dataset?  Example:  common height is about 67" tall (informal).  
- Spread. Variability.  How far away from the center do we expect individuals to be?  In what range do we expect most individuals to lie?  Example:  in our class most students are between 63" and 75" (informal). 
- Shape. Ex:  for your heights:  a little symmetric, but also a little skew right (think:  skew == tail).


Ex:  For # siblings, there is STONG right skew in the histo:  most of you only have 1-2 siblings, it's rare for you to have many siblings. 

WARNING:  The details of the shape of the histogram can vary drastically depending on the bin width / bucket size you choose.  THIS IS ARBITRARY!  It's up to you decide how to represent the data.


# Monday Feb 8

## Visualizations for categorical variables

Most important - bar graph.  Looks like a histo, but it's not!

(We never ever use pie charts to visualize categorical variables.  We don't trust anyone who does!  Sorry USA Today!)


Example:  Your hair color.  

Note:  Bar graphs can show either frequencies (counts) -or- proportions (percentages) on the y axis.  In the Google sheets example, we have frequencies.  Both of these have exactly the same shape and scale, ie, look the same!  

Key information:  what proportion of the whole belongs to each "level" of the category?  Here:  most have brown hair, few have red, etc.

All proportions together are called a "distribution".  

Differences from histogram:

- The order of the bars (categories) is totally arbitrary!  So, it doesn't make sense to talk about the "shape" of a bar graph!  Also, no notion of center or spread in a bar graph.  I.e., it doesn't make sense to talk about the "mean hair color".

- In a bar graph, the bars don't touch.  Each category is distinct and seperate.


## Text:  2.1 and 2.2 -- Measures of center and spread for quant variable


We need objective measures of center and spread.

Note:  these tools and techniques are made by people!

Whenver we're analyzing data, we must always be careful to choose the right tool for the job!


## Measures of center

Remember, the center of a dataset tells us "what's common" or "typical".

There are several measures:

1) Mean.  (Average)  Story of the formula:  the mean is "balancing point" of the data.

2) Median.  Story of the formula:  middle value.  If we ordered the values, about half are above the median, about half are below.

3) Mode.  Story:  most common element.  We don't really use mode much.


Both mean and median have different strategies for measuring center, it's not necessarily the case that one is better!  Depends on circumstance!

Ex: Consider the following silly data:  1, 1, 1, 1, 1, 19

Mean:  (1+1+1+1+1+19)/6 = 4
Med:   Average of 1 and 1 = 1.

Here, the median did a better job of measureing what's "typical" in this dataset.

Important word:  A statistic is **robust** if it's not strongly affected by skew and/or outliers.


We just saw that the median is robust, but the mean is not.  The mean was strongly affected by the outlier/skew ("19"), whereas the median isn't affected by the values on the outskirts of the distro.

Thus, if our distro has skew/outliers, we prefer the MEDIAN to measure center.  If the distro is symmetric, then we prefer the mean.  

More about this relationship:

  - If the distro has left skew, then we expect the mean to be -smaller- than the median!
  - If the distrto has right skew, then the mean is -larger- than the median.
  - If the distro is symmetric, then the mean and median are approx equal.
  
  
Example:  Consider the following variable:  employee salaries at a large company (thousands of employees), everyone from custodial staff all the way up to CXOs.

We'd expect this distro to be strong right skew -- most people make relatively little money, only a handful have very large salaries.

We'd expect the mean to be larger (maybe much larger!) than the median.  This could be used to decieve about "normal" earnings at the company.  

# Wed Feb 10

## Opening Example - Measures of Center

Ex:  Consider the exam scores in a large class on an easy exam.  

Since we'd expect most students to have high scores, only a few with low scores, we'd expect this distro to have left skew.  

Since there's skew, we use the median to describe the center of the distro.  We'd expect the mean to be smaller (it's pulled down by the left tail).


## Measures of Spread for a quant variable

There are three main ones:

### Range.  

Max - min.  Advantages:  super easy to calculate.  Con:  very, very susceptible to outliers/skew!!!  Even ONE unusual measurement/outlier messes up the range.  

###  Standard Deviation.  

Forumula notes:

x_i's are individual values
xbar = mean
x_i - xbar is the "deviation" for x_i.  Ie, how far away from the mean is x_i?
n = sample size/number of values

- We square the deviations to ensure that all are positive, and so don't cancel each other out.  In fact, the sum of the deviations (not squared) is always zero!!!  By adding square deviations, we get TOTAL variability in the dataset.

- We square rooted the squares.  Thus, std dev has the SAME units as the original data!

All together, Story of the Formula:  Stdev is the AVERAGE DISTANCE between data points and the mean!


Example:  Compute the stdev of 1, 2, 3

Here:  xbar = 2.

First, add square deviations:

(1-2)^2 + (2-2)^2 + (3-2)^2 = 1 + 0 + 1 = 2

s = sqrt(2/(3-1)) = sqrt(1) = 1.

### Interquartile Range.  IQR.

The quartiles are the medians of the lower and upper halves of the data.

Q1 = median of the lower half of the data.  Larger than 25% of the data
Q3 = median of the upper half of the data   Larger than 75% of the data

The IQR = Q3 - Q1.  Story of the formula:  The IQR is the width/range of the middle half of the data!

Ex:  71, 72, 75, 78, 80, 82, 88, 95

Lower half:  71, 72, 75, 78   ->  Q1 = 73.5
Upper half:  80, 82, 88, 95   ->  Q3 = 85

IQR = 85 - 73.5 = 11.5


Note:  Since std dev is based upon the mean, it is NOT robust!  Ie, it's affected by skew/outliers.

Since IQR only describes the middle 50% of the data, it's not affected by skew/outliers.  IQR is robust.


## Statistical Inference

So far, we've learned how to describe sample data.  Visualizations, numerical summaries, etc.

Often, our real goal is to say something smart about the POPULATION that our sample came from.  

Generally, populations are too big to take a census (ie, collect data from all individuals).  All americans, all DU students, all giraffes, etc etc.   

We're forced to analyze samples, (hopefully) representative subsets of the population.

We HAVE sample data.
We WANT info about the population.

Even though sample data is limited, imperfect, incomplete, etc, we can still make smart mathematically-supported conclusions about our population:

**Statistical Inference** is the process of making mathematically-supported conclusions about populations based on sample data.

Every numerical summary that we consider has TWO VERSIONS:

**Statistics** are numerical summaries of sampes.  Ex:  sample mean, xbar.

**Parameters** are numerical summaries of populations.  

Examples:


- Mean.
  - Sample mean.  Symbol:  xbar.  This is what we have.
  - Population mean.  Symbol:  mu.  This is what we want.
  
  Goal:  say something smart about mu based on xbar.
  
  
- Stdev.
  - Sample stdev.  Symbol:  s, s_x
  - Pop stdev.     Symbol:  sigma
  
- Proportion. (Ie, the main statistic for categorical variables.  Same as percent.)
  - Sample proportion:  phat
  - Pop Proportion:     p  (pi was busy)
  
  
  
  
  
$$\hat{p}$$
$$\sigma$$


# Mon Feb 15

## Z-scores

So far, all of our descriptive statistics tell us about entire distributions.  Ex:  mean, median, mode, range, stdev, iqr, et etc etc.


Goal for today:  what can we say about individual measurements?


Specifically:  how "unusual" (or not) is a value in a dataset.  

Main tool:  z-scores.  Also called:  normal scores, standard scores. 

For a particular observation, x, the z-score is the distance, measured in std devs, between x and xbar.  

Ie, how many std devs above (or below) the mean is x?

This is a little like "deviation":  x - xbar.  BUT, z-scores don't measure absolute difference.  They measure the distance in units of std devations.  

Z-scores account for BOTH the center and spread in a distribution.

Formula:

   z = (x - xbar)/s_x
   
   
   
Example:  The mean score on the ACT is 17, stdev 4.3.  The mean score on the SAT is 765, stdev 56.  

Amy took the ACT and scored 29.  Beth took the SAT and scored 872.  Who performed better?

Problem:  ACT and SAT have different scales!

Answer:  z-scores!

Amy:

```{r}
(29-17)/4.3
```

Amy scored 2.79 std devs above the mean.  

Beth:

```{r}
(872-765)/56
```

Beth scored 1.91 stdevs above the mean for the SAT.


Amy performed better on the test.



## Empirical Rule (68-95-99.7 Rule) 


IF a distribution follows a bell-shaped ("normal") distribution [btw, these are super common in the nature], then:

1) About 68% of all the data in the dataset lies within +/1 stdevs of the mean.
2) About 95% of data lies within +/- 2 stdevs of the mean.
3) About 99.7% of data lies within +/- 3 stdevs of the mean.


The Empirical Rule lets us "carve up" the normal/bell distribution.  

Example:  Giraffes have average height 18', stdev 2.5'.  What percent of giraffes are between 15.5' and 23'?  Giraffe heights follow a normal/bell dist. 

Z-scores!

15.5:

```{r}
(15.5-18)/2.5
```

23:

```{r}
(23-18)/2.5
```

In this range:

```{r}
34+34+13.5
```

About 81.5% of giraffes are between 15.5' and 23' tall.  


$$f(-1)=2 \textrm{, and } \lim_{x\rightarrow-1}f(x)\textrm{ does not exist}$$
$$f(0)=-1 \textrm{, and } \lim_{x\rightarrow0}f(x)=0$$
$$f(1) \textrm{ is not defined, and } \lim_{x\rightarrow1}f(x)=2$$
$$f(2)=0 \textrm{, and } \lim_{x\rightarrow2}f(x)=0$$




# Wed Feb 17

## Practice with Empirical Rule

Ex)  For men in the US ages 20-29, the mean height is 69" with stdev 2.7" (this is true).  For women, the mean height is 64", stdev 2.4" (also true).  For both, the distribution follows a normal/bell curve distribution (yep, that's true too!).

a)  If we select a man at random (rando man), what's the probability that he's between 63.6" and 71.7"?

Find z-score:

For 63.6:

```{r}
(63.6 - 69)/2.7
```

For 71.7:

```{r}
(71.7 - 69)/2.7
```

Find area of region correspondinging to z=-2 to z=1

```{r}
13.5+34+34
```

About 81.5% of men are between 63.6" and 71.7" tall.

Note:  right now, we can only solve these problems for whole number z scores.



b)  If we select a rando woman, what's the probability that she's over 71.2" tall?

```{r}
(71.2 - 64)/2.4
```

What portion of the normal/bell curve is above z=+3?

Only 0.15% of women are as tall or taller than 71.2" (5'11.2").



## 5-number summary, boxplots, and OUTLIERS

Outliers:  measurements that are substantially different from others in the dataset.  Unusual!

Goal:  mathetical criterion for what IS and outlier and what ISN'T.

### 5 number summary

These numbers:  

- low/min
- q1
- med
- q3
- hi/max

The 5 number summary divides a dataset into quarters.

Ex:  42, 71, 73, 74, 78, 82, 83, 95

5-number summary:

- low:  42
- q1 :  72
- med:  76
- q3 :  82.6
- hi :  95

## Outliers

If any value in a dataset is either:

    less than Q1 - 1.5*IQR = Q1 - 1.5*(Q3 - Q1)
    
or

    greater than Q3 + 1.5*IQR = Q3 + 1.5*(Q3 - Q1)
    
then we consider that value to be an outlier.



Ex:  42, 71, 73, 74, 78, 82, 83, 95

5-number summary:

- low:  42
- q1 :  72
- med:  76
- q3 :  82.5
- hi :  95

Are there outliers?  Let's compute the "cutoffs":
low:
```{r}
72 - 1.5*(82.5-72)
```
hi:
```{r}
82.5 + 1.5*(82.5-72)
```

Since 42 is less than the low cutoff of 56.25, we consider it to be an outlier.  There are no other outliers.


   
   
1) Range.  Max - min.
2) Stdev.  s is the average distance between points in the dataset and the mean.  
3) IQR.  It's the range of the middle 50%.  






















Statistical inference is the process of making mathematically supported conclusions about populations based on sample data.  

Ex, we could try to estimate the mean height (mu) of all DU students based on the mean height of a sample of 50 students (xbar).  


Say something smart about p based on phat.  

Say something smart about sigma based on s_x.  






















Histograms are visualizations for quantitative data.  
Bar graphs are visualizations of qualitative data.

Differences:

- In a bar graph, order of the bars don't matter.  No notion of shape, spread, center, etc.
- Bars don't touch in a bar graph - categories are discrete.  





# Monday Feb 22

## Correlation

Last time:  main visual tool for analyzing relationships between 2 quant variables:  scatterplot.  

```{r}
qplot(mpg$displ, mpg$cty)+geom_smooth(method="lm")
```

```{r}
cor(mpg$displ, mpg$cty)
```


Ex:  mpg, cty vs hwy

```{r}
qplot(mpg$cty, mpg$hwy)+geom_smooth(method="lm")
```

```{r}
cor(mpg$cty, mpg$hwy)
```


Woah!  Seems like cars that are efficient in the city are also more efficient on the highway!

Ex:  iris, sepal length vs sepal width

```{r}
qplot(iris$Sepal.Length, iris$Sepal.Width)+geom_smooth(method="lm")
```

```{r}
cor(iris$Sepal.Length, iris$Sepal.Width)
```



Looks like a blob!  Seems like no strong relationship.

## Important features/observations about scatterplots


1) Form.  Shape.  Examples:  linear, parabola, exponential, logistic, etc et...  FOr us in 120:  only care about linear.  

2) Direction.  
  - positive.  When x gets bigger, y tends to get bigger too.  Goes up!
  - negative.  When x gets bigger, y tends to get smaller.     Goes down!

3) Strength. How closely do the points seem to fit the pattern?  Is there lots of scatter?  Are they close to the line?  




## The linear correlation coefficient, r

r measures both strength and direction of a linear relationship.  

### Important properties of r:

- The sign (+/-) of r is determined by the sign of the relationship.  Pos r value means positive relationship, neg r value means neg rel.
- If r is close to +1, then there's a strong positive lin rel.
- If r == 1, the points in the plot are **colinear**
- If r is close to -1, then there's a strong negative lin rel
- If r == -1, the points are colinear
- If r is close to zero, there is little/no LINEAR relationship.
- Related warning:  r ONLY measures LINEAR relationships.  Just because r is close to zero, that doesn't mean that there's NO relationship.
- r only tells you about the strength and direction.  It DOESN'T tell you about the slope.  

- r has no units.  
- r is unaffected by the choice of units in measuring data!  Ex: inches/cm, F/C, mi/km, pounds/kg etc.  Changing units has no effect on r.  WHEW!  Don't have to worry about choice of units.
- For any dataset, it's always true that:   -1 <= r <= 1.   Ie, you can't have r=1.81.



## Linear Regression

Idea:  what's the "best" line that comes "as close as possible" to the points?

Many names:

- best fit line
- trend line
- regression line
- linear model
- least squares line (my fave)

GOOD NEWS:  these all describe exactly the same thing!  There's exactly ONE unique line that does the best job!  Whew!

Problem:  What's the equation of the line?  What makes it the best?

Goal:  minimize the (vertical) distance between the points and the line.  RESIDUALS!

    residual = point - line
             = y - yhat
             = observed - expected
             = error in prediction!
             
It's called the "least squares" line because it minimizes the SQUARE residuals:

    (y - yhat)^2
    
Need to use square residuals so that + resids and - resids don't cancel out.  Minimize TOTAL error in the model.  







# Friday Feb 19

## Boxplots

Boxplots are another visualization for quant data.  They show specifically the 5-number summary and outliers.


Example:  Scores;  42, 71, 73, 74, 78, 82, 83, 95
5-# summary:

lo:  42
q1:  72
med: 76
q3:  82.5
hi:  95

Outlier cutoffs:
low:  q1 - 1.5*(q3 - q1):
```{r}
72 - 1.5*(82.5-72)
```
hi:  q3 + 1.5(q3 - q1)
```{r}
82.5+1.5*(82.5-72)
```

There's an outlier:  42.

Ignoring the outlier, it seems that our grade distro has right skew (the right whisker is longer).  

Notes:

- Boxplots are very useful for comparing multiple quant distributions, because you can have them on the same axis!

```{r}
boxplot(iris$Sepal.Length~iris$Species, horizontal = T)
```

Let's compare!  Look like setosa has the lowest center (ie we expect setosa sepal lengths to be shorter).  

Versicolor and virigina have similar spreads, looks like virginica is a little more spread out.  

The only outlier is in the virginica species with a sepal length of about 4.9cm.

A disadvantage of boxplots:  can miss details about the shape (bimodal or multimodal).  

```{r}
hist(iris$Petal.Length)
```

### What to do about outliers?

We've seen that outliers can strongly affect our statistical analyses.  What to do?

Good practice:  analyze data two times:  with outliers and without.  Deciding how important they are is up to you, the investigator!  


## Correlation:  comparing TWO quantitative variables.

How can we study the relationship between TWO quantitative variables?

Ex:  height and weight.  We'd expect, in general, that taller people are heavier.  

As always, start with a picture!  Here:  scatterplot.

A **scatterplot** is an x-y plot of all points for our two variables.


Ex: In the mpg data, make a scatterplot show the relationship between displ (x) and cty mpg (y).


Displ:  size of the engine (liters)
cty:    mpg in city traffic


```{r}
qplot(mpg$displ, mpg$cty)
```

Woah!  Looks like larger engines tend to be less efficient!  



# Wed Feb 24

Goal: model the relationship between x and y as a line.  

We think of x as the "indendent variable", or "explnatory variable."

We think of y as the "dependent variable", or the "response variable."

Our linear model describes the effect that x has on y.  We make prediction ABOUT y BASED UPON x.

Linear model, ie line.  All lines have the same form of equation:

    y = mx + b
    
where

  - m is slope
  - b is intercept
  
  
For us, no different.  BUT, use differen notation (letters):


     yhat = b0 + b1*x
     
where

  - b0 is the intercept
  - b1 is the slope
  
  
  
## Finding eqn of the least squares line

1) Slope

    b1 = r*sy/sx
    
where:

  - r is the correlation coeff
  - sy is the stdev of the y data
  - sx is the stdev of the x data

Stories about the formula:

- Since teh slope is scaled (multiplied by) r, stronger correlation means bigger slope.  I.e., if there's stronger correlation, then x has a steeper effect on y!

- Remember from HS:  m = change in y / change in x.  Here, we're looking at the ratio of standard deviation (think: variability) in y compared to x  (sy/sx).

2)  Intercept

Neat fact:  the regression line ALWAYS touches the point (xbar, ybar).

From this, get intercept:

    b0 = ybar - b1*xbar


Silly example:  Suppose that xbar = 1, sx = 2, ybar = 3, sy = 4, r = 1/2

b1 = r*sy/sx = (1/2)*4/2 = 1

b0 = ybar - b1 xbar = 3 - 1*1 = 2

->  yhat = 2 + 1*x


## Interpreting Coefficients

Coeffiecents: b0 and b1

Goal:  study rel btwn x and y.  

Slope:

From high school:  change in y / change in x.  If x increases by 1, then y changes by the slope.  

In our linear model, we're making predictions about y based on x.  In context:


If the [x variable] increases by 1 [x units], then we predict/expect [y variable] to change (inc/dec) by [slope] [y units].


Ex:  In a sample of 50 men, the average height was computed to be 68.7" with stdev 3.1".  Their avg weight was 174.3 lbs with stdev 10.2.  The correlation coefficient was computed to be 0.764

Find the equation for the linear model to predict weight based on height.  Interpret the slope of this model.


x = height
y = weight

xbar = 68.7
sx   = 3.1
ybar = 174.3
sy   = 10.2
r    = 0.764

1) slope:

```{r}
0.764*10.2/3.1
```


2)  intercept:

```{r}
174.3 - 2.514*68.7
```

    yhat = 1.588 + 2.514x
    
    
Slope = 2.514.  Interpret:  For each addition inch tall a man is, we'd predict/expect that his weight increases by 2.514 lbs.




Ex:  Iris.  Predict pedal width based on petal length.  Eqn:  yhat= -0.363 + 0.416x

Interpret slope:  For additional cm longer a petal is, we predict/expect its width to increase by 0.416 cm.

### Interpreting y-int

From HS:  the y-intercept is the y-coord where x = 0.


In a lin model, the intercept is what we predict [y variable] to be when [x variable] is zero [x units].

Ex:  height model.  yhat = 1.588 + 2.514x

x = height
y = weight

If a man is 0 inches tall, we'd predict that he weighs 1.588 lbs.  Of course, this is silly!  Extrapolation!  Ie, making predictions based on x-vals that are outside the range of observed values.  Bad!


- Just because we observed a relationship in our data, there's no reason to assume that it continuous OUTSIDE the range of observations we've made!

- "Garbage in, garbage out!"

Ex:  iris.  Petal width based on petal length.

    yhat= -0.363 + 0.416x
    
Interpret intercept:  if a petal's length is 0cm, then we predict its width to be -0.363cm.  (!!!)

Extrapolation!


```{r}
qplot(iris$Petal.Length,iris$Petal.Width)
```


##  Evaluating the model

Idea:  is it good?  is it bad?  can we trust predictions made by the model?


### Effects of outliers

Outliers can have a variety of effects on the linear model.  None of the coefficients are robust, so we wonder:  are the outliers changing the model?

Ex:  Iris.  Predict sepal width based on sepal length

```{r}
qplot(iris$Sepal.Length, iris$Sepal.Width)
```

Looks like a blob.  r:

```{r}
cor(iris$Sepal.Length, iris$Sepal.Width)
```

What if we add the following outlier:  x=40, y=40

Moral of the story:  the original data DID NOT have a strong relationship.  Here, the outlier made a weak relationship SEEM stronger than it really is!





# Monday Mar 1

## Evaluating the Model

### Effects of outliers (ctd)

Ex:  mpg, x=displ, y=cty

```{r}
qplot(mpg$displ, mpg$cty)
```
```{r}
cor(mpg$displ, mpg$cty)
```

What if outlier:  x=20, y=60

Correl before: r=-0.8, cor after = -0.06.  Moral of the story:  sometimes, an outlier can make a strong relationship seem weak!


**Leverage** is how much imapact an outlier is likely to have on the model coefficients.  Outliers with x-coords far outside the range of observations (extrapolation!) tend to be high leverage.  Example:  both we've seen so far.

Outliers with x-val within the range of "normal" observations tend to be low leverage.  Example:  below.

Example:  mpg.  x=displ, y=cty.  (same as above, see scatterplot)

Outlier:  x=4.5, y=50

Correl before: r= -0.8.  Correl after:  r = -0.7.  Some impact, but not much, since this isn't a "high leverage" outlier.

Ex:  mpg, x=cty y=hwy

```{r}
qplot(mpg$cty, mpg$hwy)
```

Outlier:  x=100, y=125

Here, started with strong pos rel, outlier doesn't change that since it fits the pattern.  Moral:  sometimes outliers don't have any effect on the liner model.

BIG ULTIMATE MORAL:  Outliers can do lots of things!  Need to check the scatterplot to decide.


## Evaluating the Model (ctd)

### Residuals

Recall:

    resid = y - yhat
          = obs - expected
          = point - line
          = vertical distance between the point and the line
          = ERROR!!!!  BAD!!
          
First:  residual plot!

   x-axis:  original x data
   y-axis:  residuals!
   
Example:  mpg, x=displ, y=cty

Original:

```{r}
qplot(mpg$displ, mpg$cty)+geom_smooth(method="lm")
```

When analyzing a residual plot, the following suggest that the linear model is not the best/not appropriate (bad residuals):

- A pattern.  (Usually a curve pattern).  If the residuals have a pattern, then the errors are predictable!  But, if errors are predictable, we should be able to fix them!  Linear model probably not the best.

- heteroskedasticity.  magnitude of residuals changes across the range of x vals.  If residuals are heterosked, then the model does well in some ranges, but poorly in others.  Linear model probably not the best.

"Good residuals":  evenly dispersed, no patter.  Blob!  No pattern in error -> can't do better than linear model.  Lin model is good!

### We hope residuals follow normal dist

To see:  histogram of residuals!

```{r}
mpgmodel <- lm(mpg$cty~mpg$displ)
hist(mpgmodel$residuals)
```

Not awful, but definitely some right skew.  Could indicate that linear model isn't the best.



# Wed Mar 3




## Evaluating the model (cont)

So far:

- outliers
- residuals (resid plot and histogram of resids)


Now:  Coeffecient of determination, r^2  (or R^2).

Goal in lin reg:  study effects of x on y.  Genenerally, we know x isn't the only factor!  Ex:  predicting weight based on height.  We know height's not the only factor, but how much does it account for?

Answer:  The coefficient of determination is the percentage of variation in the y data that's due to/because of/attributable to the linear relationship between x and y.  

Ex:  Suppose data is collected on 50 men.  We calculate the correlation coefficient between height and weight to be r = 0.78.  For the linear model that predicts weight based on height, calculate and interpret the coefficient of determination.  

x = height
y = weight

Calculate r^2:
```{r}
(.78)^2
```

Of all the variation in their weight, about 61% was because of the effect of the linear relationship between height and weight.  


Ex)  mpg data, x=displ y=cty

```{r}
cor(mpg$displ, mpg$cty)
```

r^2:

```{r}
(-0.798524)^2
```


Interpret r^2:  Of all the variation in city fuel efficiency, about 64% is attributable to the relationship between engine size and fuel efficiency.  











## Ch 3 - Probability

### Two Defs

A chance experiment or probability experiment is any occurance that could result in at least two different ways (possibly many more!), and there's no way of knowing in advance what the outcome will be.


Ex:  A coin toss.  Results:  either heads or tails.

Ex:  Roll a dice.  Results:  1,2,3,4,5,6

Ex:  Pick two cards at random from a deck.  Results:  all possible combos of 2 cards

Ex:  Select a DU student at random, record their year in school.  

Ex:  Select a random sample of 35 DU students, record their average height.


Goal:  decide how likely or unlikely certain outcomes are!

Answer:  Probability

1)  Classical/Theoretical probability of an outcome is the ratio of occurances of that outcome after observing the random process an infinite number of times.

Ex)  A fair coin is tossed.  What's the probability of a H showing up?  Two possible outcomes, both equally likely:

    P(H) = 1/2 = 50% = 0.5
    
    
2)  A fair 6-sided dice is rolled.  What's the probability of observing a number greater than 4?  Two numbers greater than 4, 6 possibilities total:

     P(>4) = 2/6 = 1/3 = 0.33333 = 33.33333%
     
     
PROBLEM:  can't always compute theoretical probabilities!  Could be very large number of possibilities (all humans), could be infinite!, could be changing!

2) Experimental/Observational/Relative Frequency probability is ratio of times (relative frequency) that an outcome occurs over the total number of trials (n).


Ex:  An imprisoned mathematician flips a coin 10,000 times and observes 4967 heads.

     P(H) = 4967/10000 = 0.4967
     
Good!  We can always compute relative frequencies!

Bad!  They're usually wrong!  (At least a little)

### The Law of Large Numbers

As the number of repetitions, n, of a random process increases, the relative frequency probability tends to get closer and closer to the true theoretical probability.

Good news!  We don't have to care too much about the difference between theoretical and experimental probabilities, as long as n is "large enough".  

Important notes:

- When n is small, we tend to see lots of variability in experimental probability.

- When gets large, two things:
  1) Variation gets small (repeatable outcomes)
  2) The value they converge to is the true/theoretical probability
  
  
Ex:  Goal:  find P(1) when rolling a dice via experimental prob.  

```{r}
186142.4 + 39.113*1420
```


















### Disjoint Events / Addition Rule

A random process can have many possible outcomes.

An "event" is any collection of those outcomes that we're interested in.

Ex:  Roll a dice.  Let event A = the event that an even number shows.

    A = {2,4,6}

Two events are "disjoint" (mutually exclusive) if they have no outcomes in common.

Ex:  Roll a dice.  Are the events disjoint?

     A = even number
     B = prime number
     
No, not disjoint, since 2 is an outcome in both!

**Addition Rule for disjoint events**:  If A,B are disjoint events, then:

     P(A or B) = P(A) + P(B)
     
Think:  If disjoint, then "or" means "+"

Ex:  Roll a dice.  

    A = an odd shows up                 = {1, 3, 5}
    B = number greater than 5 shows up  = {6}
    
Find P(A or B):

    P(A or B) = P(A) + P(B)
              = 3/6  + 1/6  = 4/6
    
    
 







### Not Disjoint Events (General Addition Rule)

For ANY two events A,B:

     P(A or B) = P(A) + P(B) - P(A and B)
     
Idea:  don't want to double count the overlap (A and B)


Ex)  Roll a dice.  

     A = even
     B = prime
     
Compute P(A or B).

    P(A or B) = P(A) + P(B) - P(A and B)
              = 3/6  + 3/6  - 1/6
              = 5/6
              
Ex:  **Contingency Table**/**Two Way Table**

```{r}
table(mpg$year, mpg$drv) %>% addmargins()
```


a)  Compute P(f).  Ie, if we randomly select a car, what's the prob that it's a front-wheel-drive car?

    P(f) = 106/234
    
b)  Compute P(99 or r).

    P(99 or r) = P(99) +   P(r) -   P(99 and r)
               = 117/234 + 25/234 - 11/234
               
```{r}
117/234 + 25/234 - 11/234
```
               


Ex)  A card is selected at random from a 52-card deck.  What's the probability that it's either a red card or a face card?

    P(R or F) = P(R)  + P(F) -  P(R and F)
              = 26/52 + 12/52 - 6/52 = 32/52
              
```{r}
32/52
```
              




### Probability Distribution

A prob dist is:

- list of all outcomes
- for each, the associatied probability

Usually shown as a table.

Ex:  Two coins are tossed.  We record the number of heads showing (x).  Construct a prob dist for x.


H, H   ->  x=2
H, T   ->  x=1
T, H   ->  x=1
T, T   ->  x=0


 x   |  0  |  1  |  2  |
 P(x)| 1/4 | 2/4 | 1/4 |


# Mon March 8

Ex:  Roll two dice.  Let X=sum of the faces shown  Construct a prob dist for x.

- What are possible values for x?

smallest:  x=2
biggest:   x=12

Possible values for x:  2, 3, 4, .... 12.  

"Support":  the set of possible values for x.  

How many possible outcomes?  

One outcome:  (3,5)  [three on first, five on second]  x=8

__| 1 | 2 | 3 | 4 | 5 | 6
1 |
2 |
3 | 
4 |
5 | 
6 |

How likely is each outcome?


x = 2  (1,1)                                       P(x=2) = 1/36
x = 3  (1,2), (2,1)                                P(x=3) = 2/36
x = 4  (1,3), (3,1), (2,2)                         P(x=4) = 3/36
x = 5  (1,4), (4,1), (3,2), (2,3)                  P(x=5) = 4/36
x = 6  (1,5), (5,1), (4,2), (2,4) (3,3)            P(x=6) = 5/36
x = 7  (1,6), (6,1), (5,2), (2,5), (3,4), (4,3)    P(x=7) = 6/36
x = 8  (2,6), (6,2), (5,3), (3,5), (4,4)           P(x=8) = 5/36

P(x = 9) = 4/36
P(x = 10) = 3/36
P(x = 11) = 2/36
P(x = 12) = 1/36

x   | 2     3     4     5     6      7      8     9     10      11     12
P(x)|1/36 2/36  3/36  4/36   5/36   6/36   5/36  ....




####  Properties of Prob Dists

1)  0 <= P(x) <= 1  for all x
2)  sum(P(x)) = 1  (for all x)









### Complement

The complement of an event A 

(Notation:  A',  A^C)

is the set of all outcomes NOT CONTAINED in event A.

Ex:  Roll a dice.  A = event that an even shows.

Options:  {1,2,3,4,5,6}

A = {2,4,6}

A' = {1,3,5}

#### Complement Rule:

     P(A')  = 1 - P(A)

Makes sense: any outcome is either in A or not in A, so:

    P(A) + P(A') = 1
    
    
Ex:  Flip a coin 4 times.  What's the probability that at least one coin shows H?

Note:  many possible outcomes!

H T H T
H H H H
H T T T
H T T H

....


Complicated!  But, the complment is simple:

T T T T

    P(T and T and T and T) = (1/2)*(1/2)*(1/2)*(1/2) = 1/16
    P( at least one heads) = 1 - P(all are tails)
                           = 1 - 1/16 = 15/16
                           
```{r}
15/16

```
                           
                          
Ex:  About 19% of DU students are international students.  If we take a random sample of 10 students, what's the probability that at least one of them is an international student?

Complement:  none of them are international.  

    P(all 10 are not international) = (.81)*(.81)*(.81)*...*(.81) = (.81)^10
    ->  P(at least one is international) = 1-(.81)^10
    
```{r}
1-(.81)^10
```
    
#### "At least one" problems:

     P(at least one) = 1 - P(none)
    
    
    



### Independence/ Mult Rule

Recall:  A,B are disjoint if they have no outcomes in common.  If so:

     P(A or B) = P(A) + P(B)
     
Think:  if disjoint, "or" means "plus"
     
Two events A,B are **independent** if the outcome of one doesn't affect the likelihood of the other.

WARNING:  TOTALLY DIFFERENT FROM "DISJOINT"!

Ex:  Event A:  it's cloudy.  Event B:  it's raining.  Probably not independent!  These events are "dependent".  

#### Multiplication Rule for Indep Events:

If A, B are indep, then:

    P(A and B) = P(A)*P(B)

If indep, then "and" means "times".


Ex:  flip a coin 5 times.  

a)  P(H, H, T, H, T) = (1/2)*(1/2)*(1/2)*(1/2)*(1/2) = (1/2)^5 = 1/32

Ex:  We randomly select three cards from a deck, and replace them at random each time we choose one.  What's the probability that all three are hearts?

    P(H and H and H)  = (13/52)*(13/52)*(13/52)

```{r}
(13/52)*(13/52)*(13/52)
```


Indep events!

What if we DON'T replace the card?


    P(H and H and H) = (13/52)*(12/51)*(11/50)
    
```{r}
(13/52)*(12/51)*(11/50)
```
    
Hmm.... both seem similar...





### Sampling w/ Replacement vs Not

Saw:  often similar!

Idea:  If population is "large", it doesn't really matter if we sample with replacement or not!

Rule of thumb:  if the sample size is no greater than 5% of the pop size, we consider all samples to be independent.  YAY!  Multiplication rule for indep events is easy!


Ex:  We take a sample of size n=50 from the pop of DU students (2300).  

```{r}
50/2300
```

Sample n is <5%, so we consider indpendent!  Yay!  Doesn't matter if we sample with replacement!

Ex:  If 51% of DU students are female, and we take a sample of 10 students, what's the probability that they're all women?

Note:

```{r}
10/2300
```

Assume independent, use multiplication rule:

.51*.51*.51*......*.51 ...

```{r}
.51^10
```






### Conditional Probabity 

    P( A | B ) = "prob of A given B"
               = "if we know B occurs, how likely is A?"

Ex:  P( Rains | Cloudy) = ?

Formula:  only considering outcomes in B (new sample space!).  So formula:

    P(A | B) = P(A and B)/P(B)
    
Think:  "and on top, given on bottom"

Ex:  At DU, 51% of students are women.  Suppose that 17% of all students are both women and STEM majors.

If we choose a random woman, what's the probability that she's a STEM major?

GIven:

   - P(W) = .51
   - P(W and S) = .17
   
Need to find:

    P( S |  W ) = P(W and S) / P(W)
                =    .17 / .51

```{r}
.17/.51
```

Ex:  mpg

```{r}
table(mpg$year, mpg$drv) %>% addmargins()
```

a) If we select a car at random from the year 1999, what's the prob that it's 4wd?

Need:  

    P( 4  | 99)  = P(4 and 99) / P(99)
                 =    (49/234) / (117/234)
                 =          49 / 117
                 
```{r}
49/117
```
                 
b) Are the events "has front wheel drive" and "is from 2008" independent?

How to check?  

Answer:  if they are indpendent, the mult rule must work!

    P(f and 08) = P(f)*P(08)
    
Is this true?  Lets check!

    P(f and 08) = 49/234
    P(f)        = 106/234
    P(08)       = 117/234
    
"and":
```{r}
49/234
```

"times":
```{r}
(106/234)*(117/234)
```

No!  P(A and B) not equal P(A)*P(B).  Not independent!












### General Multiplication Rule

Recall:

If A,B disjoint:

    P(A or B) = P(A) + P(B)
    
For any A,B:

    P(A or B) = P(A) + P(B) - P(A and B)
    
    
If A,B are indep:

     P(A and B) = P(A)*P(B)
     
Q:  What if A,B are NOT independent?  How to find "and" prob????

A:  Conditional probability!

    P(A | B ) = P(A and B)/P(B)
    
Little algebra:

    P(A and B) = P(B)*P(A|B)
               = P(A)*P(B|A)
               
Both ways work for ALL events!  Careful:  might only have one conditional prob!


Ex:  I am walking my dog.  If he sees a squirrel, there's a 82% chance that he'll bark.  If he doesn't see a squirrel, there's a 27% chance that he'll bark.  In my neighborhood, there's an 76% chance that he'll see a squirrel.

On a random walk, what's the prob that my dog sees a squirrel and barks?

Given:

   - P( B | S)  = .82
   - P( B | S') = .27
   - P(S)       = .76
   
Need to find:  

   - P(B and S) = P(B)*P(S|B)   <- don't know this stuff!
                = P(S)*P(B|S)  = (.76)*(.82)

```{r}
(.76)*(.82)
```






# Friday March 12

Ex:  Suppose P(A) = .3, P(B) = .4.  Compute P(A or B) if....

a) ... P(A and B) = .05

    P(A or B) = .3 + .4 - .05 = .65

b) ... A,B are indep

    If A,B indep, then P(A and B) = (.3)*(.4) = .12
    
    ->  P(A or B)  = .3 + .4 - .12 = .58

c) ... A,B are disjoint

    If A,B are disjoint, then P(A or B) = P(A) + P(B)  
    -OR-, if disjoint, then P(A and B) = 0
    
    ->  P(A or B) = .3 + .4 - 0 = .7



d) ... P(A|B) = .05

    P(A and B) = P(A) * P(B|A)
               = P(B) * P(A|B)  = .4*.05 = 
               
```{r}
.4*.05
```
               
    P(A or B) = .3 + .4 - .02
              = .68



Ex)  Suppose we roll a dice 15 times.  What's the probability that at least one time shows "4"?



Complement:  none are 4s!

not a 4 * not a 4 * not a 4 * ...... * not a 4

= (5/6)^15

     P(at least 1 "4") = 1-(5/6)^15

```{r}
1-(5/6)^15
```




### Trees, Total Probability, and Bayes Theorem


Ex:  A lie detector test advertises that it's "95% accurate".  This means:  if a person is lying (L), then there's 95% chance that the test shows positive (+).  

Additionally, if a person is not lying, there's an 88% chance that the test shows -.

Most people are honest:  only about 1% of people in the pop are liars.

Draw a prob tree to summarize this scenario.

P(+|L) = .95
P(-|L) = .05
P(+|L') = .12
P(-|L') = .88


```{r}
.99*.12
```


```{r}
.99*.88
```


Follow-up QUestions:

1) What's the probability that a rando person tests +?

Only two possibilities:

 - Could be honest and positive (L' and +)
 - Could by lying and positive (L and +)
 
 ->  P(+) = P(L and +) + P(L' and +) = .0095 + .1188
 
```{r}
.0095 + .1188
```
 
 
 This is the Law of Total Probability!
 
2)  If you get a + result, what's the chance that you're actually lying?

    P( L | + ) = P(L and +)/P(+)  = .0095/.1283
    
```{r}
.0095/.1283
```
    
This is Bayes Thm! Ie, reversing conditionality:

Know:  P(A|B),
Want:  P(B|A)  


Ex:  In a soccer match, a kicker is trying score a goal.  The goalie is trying to stop them.

- If the kick is to the L, there's an 82% chance of B
- If the kick is to the C, There's a 90% chance of a B
- If the kick is to the R, there's a 60% chance of a B

Kicker: 

- There's a 55% chance to kick L
- There's a 20% chance to kick C
- There's a 25% chance to kick R

1)  Make a prob tree

2)  What's the probability the ball is blocked?

   P(B) = P(L and B) + P(C and B) + P(R and B) 
        = .55*.82 + .2*.9 + .25*.6

```{r}
.55*.82 + .2*.9 + .25*.6
```

3) If the kick is to the right, what's the prob that it's blocked?

.6

4) If the kick is blocked, what's the prob that it was kicked to the right?

    P( R | B)  = P(R and B) / P(B)
               = (.25*.6)/.781

```{r}
(.25*.6)/.781
```


## HW:  Contingency Tables ( Pivot Tables )


Ex:  Student Data.  Make Pivot table for Year in School and Hair Color

   Blonde Brown Red Other Black
F
S
J
Sr

Ex:  Student Data.  Make Pivot table for height and Year In School


     Mean Height     Min Height     Max Height    Q1 Height   Q3 HEight
F
S
Jr
Sr












Suppose that a dataset about  house sales is collected, and a model to predict the sale price based upon square feet is constructed.  The equation of the model is:

yhat = 186142.4 + 39.113x

The correlation coefficient is 0.650.

Write a complete English sentence to carefully and completely interpret the slope in context of the model, as done in class.


For each additional square ft larger a house is, we expect its sale price to increase by 39.11 dollars.  











Suppose that a dataset about  house sales is collected, and a model to predict the sale price based upon square feet is constructed.  The equation of the model is:

yhat = 186142.4 + 39.113x

The correlation coefficient is 0.650.

If a particular house sold for $200,000 and had 1420 sq ft, calculate the residual for this house and explain its meaning.


    resid = y - yhat 
    
    y = 200,000
    
yhat:

```{r}
186142.4+39.113*1420
```

resid:

```{r}
200000-241682.9
```

OUr model OVER-APPROXIMATED the value of the house (negative resid) by $41,682.










```{r}
iris %>% group_by(Species)%>%summarise(mean(Sepal.Length))
```

































A researcher collected data about dogs and their lifespans.  Among the 50 dogs in his dataset, the average (adult) weight was 45.2 lbs with stdev 18.6 lbs.  The average age at death of the dogs was 12.3 years with stdev 2.1 years.  The correlation coefficient between the two variables was -0.68.

Find the complete equation for the linear model that predicts the age at death of a dog based upon its weight.

(Show at least your setup work for credit.)


xbar = 45.2
sx = 18.6
ybar = 12.3
sy = 2.1
r = -0.68

```{r}
-.68*2.1/18.6
```

```{r}
12.3-(-.0768)*45.2
```

yhat = 15.771 - 0.077x










```{r}
 0.68^2
```


x = weight in lbs
y = age in years

Of all the variation we observed in age at death, about 46.2% of it was due to the linear relationship between weight and age at death.  










P(poker or pizza) = P(poker) + P(pizza) - P(poker and pizza)
                  =  25/115  +  44/115  - 10/115
                  


```{r}
(25+44-10)/115
```


















# Mon March 22



Pivot Table Example:  Check for outliers in sepal length by species.

```{r}
boxplot(iris$Sepal.Length~iris$Species)
```

Cutoffs:

low:  q1 - 1.5*(q3-q1)
hi :  q3 + 1.5*(q3-q1)




## The Normal Dist


Idea:  We know what prob dists are.  Here's the most important one:  normal!

Why normal?  Most individuals are close to "average", its increasingly rare to be far above or below.  Pops up in nature all the time!  

Common example:  biometrics (measurements of animals) tend to be normally distributed.  Think:  human height!  

"The" normal dist is actually a family of distributions.  All have same shape, but differ in center and spread.

Normal dists are determined by their mean and std dev!

Ex:  Height for men in the US has mean 69", stdev of 2.7".
     Height for women in the US has mean 64", stdev of 2.4".  


To find probabilities for normal dists, need to find AREA!  



    AREA = PROBABILITY!!!
    
    
PROBLEM:  There's no "closed form" solution for finding this area. 

Two ways:

1)  Statistical Tables.  Look up the area!
2)  Software.  Like Google Sheets, Excel, R, SASS, ....

First:  table.  All tables show THE STANDARD NORMAL DIST.  Ie, the normal dist with 

 - mean = 0
 - stdev = 1
 
Problem:  what if our data doesn't follow a "standard" normal dist?  Ex: human height?


To convert:  find the z-score!  Z-scores measure the number of stdevs above or below the mean:


     z = (x - mean)/stdev

NOTE:  Z-SCORE == STD NORMAL DIST!!!  


To use the table:

1)  Look up your z-score
2)  Find your area

The table always shows area TO THE LEFT.  

Ex:  Suppose Z has std normal dist.  Compute:

a)  P(Z<1.23) = .8907

b)  P(Z>1.23) = 1 - P(Z<1.23) = 1 - .8907 = 

```{r}
1 - .8907
```





# Wed March 24

Last time:

P(0.72 < Z < 2.34) = P(Z < 2.34) - P(Z < 0.72)
                   = .9904 - .7642

```{r}
.9904 - .7642
```



## Reading the table Backwards

So far:  Given a z-score, find area.  Forwards!

Now:     Given an area, find the z-score.  Backwards!

Ex:  What z-score marks the 80th percentile?

In table, find area = 0.8.  Z = .84 (best option)

Ex:  What's the z-score for the 60th percentile?

Find area = .6, z = 0.25.

##  Practical problems - Forwards

Idea:  X has normal dist.  Compute z-score, look it up!

     z = (X - mean)/stdev
     
     
Ex)  Height for adult women in US follows normal dist with mean 64 and stdev 2.4.  Compute the probability that a rando woman....

a)  is less than 70" tall.

z-score:

```{r}
(70-64)/2.4
```

     P(X<70) = P(Z<2.5) = .9938
     
Ie, 99.38% of adult women in the US are 5'10" or less.



b)  P(60 < X < 65) = P( -1.67 < Z < 0.42) = .6628 - .0475

z-scores:

```{r}
(65-64)/2.4
```

```{r}
(60-64)/2.4
```


```{r}
.6628 - .0475
```

About 61.53% of women are btwn 5'0" and 5'5".  



c)  P(X>75)  =  P(Z>4.58) ~ 0

z-score:

```{r}
(75-64)/2.4
```


d) P(X<75) ~ 1


MORAL:  Just because z-score is off the charts, doesn't mean you can't find probability!


## Practical examples - backwards

IQ has normal dist with mean 100, stdev 15.  

a)  How high would your IQ have to be to be in the top 10%?  

Idea:  first, find z-score for upper 10%.  Then, solve for x.  

     z = (x-mean)/stdev
     x = z*stdev + mean


Find area = .9, z = 1.28

Solve for x:

    1.28 = (x - 100)/15
    
    x = 1.28*15+100
    
```{r}
1.28*15+100
```
    
In order to score in the top 10% of IQ, you'd need to score at least 119.2 points.  

Ex)  What's the cutoff for the 25th percentile of IQ?

Look up area = .25, z = -.62

Solve for IQ:

      -0.62 = (x-100)/15
      
      x = (-.62)*15+100
    
```{r}
(-.62)*15+100
```
    
The bottom quartile cutoff for IQ is 90.7 points.  
    
    
# Friday March 26

## "Middle Percent" Problems

Ex)  Height for men is normal with mean 69" stdev 2.7".

Q:  What's the range of the middle 50% of heights for men?

Note:  backwards!  Given area, need ranges of heights.


Need to look up area.  But, what area?

For lo cutoff, look up area = 0.25, z = -0.67

For hi cutoff, look up area = 0.75, z = +0.67

Find heights:

lo:
     z = (x-mean)/stdev
     -0.67 = (x-69)/2.7
     
```{r}
-.67*2.7+69
```

hi:
```{r}
.67*2.7+69
```

Cutoffs for middle 50%:  from 67.2" to 70.8".  

Range:

```{r}
70.8-67.2
```

Note:  This is IQR!  Range of middle 50%!

Ex)  Mean IQ is 100, stdev 15, normal.  Find range of middle 90% of IQ.

Find area = .05, both z=1.64 and 1.65 are equally good, split the diff:  

     z = +/- 1.645

Cutoffs:  

      x  = z*stdev+mean
      
```{r}
-1.645*15 + 100
```
     
```{r}
1.645*15 + 100
```
 
Range between 75.3 and 124.7:

```{r}
124.7-75.3
```

CUTOFF FOR QUIZ.  Quiz:  normal dist!  forwards and backwards!  basic and practical!  need z-table  


## Sampling Distributions

So far: asking questions about individuals.  

A "sampling dist" is a probability distribution for the likelihood of a SAMPLE STATISTIC.

- How likely is our sample mean?        xbar
- How likely is our sample propotion?   phat
- How likely is our sample stdev?       s_x
- How likely is our cor coef?           r

Ie:  NOT ABOUT INDIVIDUALS!  Instead, describing ENTIRE SAMPLES!


Ex)  Suppose we roll two dice.  Find the sampling dist for their mean, **xbar**.

Need two things:

- All possible values of xbar (all possible means of two dice)
- Find their probabilities



xbar = 1   (1,1)
xbar = 1.5 (1,2), (2,1)
xbar = 2   (1,3), (3,1), (2,2)
xbar = 2.5 (1,4), (4,1), (2,3), (3,2)
xbar = 3   (1,5), (5,1), (2,4), (4,2), (3,3)
xbar = 3.5 (1,6), (6,1), (2,5), (5,2) ,(3,4), (4,3)  
xbar = 4   (2,6), (6,2), (3,5), (5,3), (4,4) 
..
xbar = 6   (6,6)

Picture!  Observations:

1)  Shape has changed!  X has a "flat" distribution, Xbar has a peak in the middle

2)  Spread has changed!  X has larger spread than xbar!

3)  Center has not changed!  

These are always true for ALL sampling dists, no matter what x is, no matter what n (sample size):

 - shape becomes more peaked in the middle (sound familiar?)
 - spread smaller as n bigger
 - center never changes!
 
 
 
# Wed March 31


Important properties of sampling dists:

- center doesn't change.  

      mu_xbar = mu_x
      
      
      
- stdev decreases

      sigma_xbar = sigma_x / sqrt(n)
      
Note:  as n increases, spread for xbar decreases!

- if X (original pop) is normal, then xbar is normal no matter what, even for small n

- As long as n is "large enough", then xbar follows a normal distribution NO MATTER WHAT POPULATION WE'RE SAMPLING FROM!  ANY X!  

This is the Central Limit Threorem!

Notes:

  - This works for many statistics, not just xbar.  Ex:  phat!  
  - For xbar, n is "large enough" if n>=30.  


Main idea:  we can say how likely or unlikely a sample result is!!


To use the CLT:

    z = (obs - exp)/spread
      = (obs - mean)/stdev
      = (xbar - mu)/(sigma/sqrt(n))
      
      
Ex)  Men's heights:  normal, mean=69, stdev=2.7.  

   mu_x = 69,   sigma_x =2.7
   
a)  What's the prob that a random man is above 70"?  

   z = (70 - 69)/2.7

```{r}
(70 - 69)/2.7
```

On table:

```{r}
1-0.6443
```
About 35.6% of men are at least 70".

b) What's the prob that, in a sample of 4 men, their avg height is above 70"?

    P(xbar >= 70)

z-score for  xbar=70:

```{r}
(70-69)/(2.7/sqrt(4))
```

Look up:

```{r}
1-.7704
```

c) What's the prob that, if n=10, xbar is at least 70?

```{r}
(70-69)/(2.7/sqrt(10))
```
Look up:

```{r}
1-0.8790
```


d)  If n=50, what's the prob their avg height is at least 70?

```{r}
(70-69)/(2.7/sqrt(50))
```

Look up:

```{r}
1-0.9956
```

Note:  prob of being away from the mean decreases (a lot) as sample size gets bigger!


Ex:  A partical model of lightbulb has mean lifespan of 10,000 hours with stdev 500 hours.  If we take a sample of 60 lightbulbs, what are the cutoffs for the middle 70% most common sample average lifespans?


Look up area = .15:  z = +/- 1.04

Find cutoffs:

    z = (xbar - mu)/(sigma/sqrt(n))
    
Lo:

    -1.04 = (xbar - 10000)/(500/sqrt(60))
    
    xbar = -1.04*(500/sqrt(60))+10000
    
```{r}
-1.04*(500/sqrt(60))+10000
```
    
Hi cutoff (same, but positive z):

```{r}
1.04*(500/sqrt(60))+10000
```


If we take sample of size n=60 lightbulbs and find average lifespan,  the middle 70% most common sample means lie between 9932.9 hours and 10067.1 hours.

# Wed April 02

## Samling dist for xbar

Center:  stays the same

    mu_xbar = mu_x
    
Think:  the mean of the mean is the mean.  

Spread:  gets smaller with n

    sigma_xbar = sigma_x / sqrt(n)
    
Shape:

  - If original pop (x) is normal, then xbar is normal no matter what (even for small n).  
  
  - CLT:  If n is "large" (n>=30), then xbar is normal no matter what x (original pop) is!  
  
  
Ex) Giraffe height is normal, mean 18', stdev 2.5'.  If we take a saple of 5 giraffes, what's the prob that their mean height is at least 19'?

mu_x = 18
sigma_x = 2.5

n = 5

Sample:

mu_xbar = 18 
sigma_xbar = 2.5/sqrt(5)

Q:  How do we know we can use Z?  Is xbar normal?  Yes, since original population (heights) already normal.  Xbar is normal no matter what n.  

P(xbar>19)

z-score:

    z = (obs - exp)/stdev
      = ( 19 - 18 )/(2.5/sqrt(5))

```{r}
( 19 - 18 )/(2.5/sqrt(5))
```

z = .89

Area to the right:

```{r}
1-0.8133
```

There's about an 18.67% chance of observing such a sample.

## Samp dist for phat

Idea:  what if we're looking at categorical data?  

Key statistic:  sample proportion, phat.  

Sampling dist for phat:

Center:  stays the same!

    mu_phat = p (population proportion!)
    
Spread:  decreases with n

    sigma_phat = sqrt( p*(1-p) / n )
    
    
Shape:  CLT works for phat also!

If n is "large enough", then phat follows a normal distribution no matter what population we're sampling from.

Large enough:  need BOTH

     n*p >= 10   and   n*(1-p) >= 10
     
Same thing:  at least 10 in the category, and 10 out of the category.  


Ex) At DU, 19% of students are international.  If we take a sample of 100 students, what's the probability that at least a quarter (.25) of students are international?

Need:   P(phat >= .25)

    p = .19
    n = 100
    
    mu_phat = .19
    
    sigma_phat = sqrt( .19*(1-.19)/100 )

Q:  how do we know we can use Z?  Is phat normal?

```{r}
100*.19
```

```{r}
100*(1-.19)
```

Yes!  n is "large enough"!  

z score:

    z = (obs-exp)/stdev
      = (.25 - .19)/sqrt(.19*(1-.19)/100)

```{r}
(.25 - .19)/sqrt(.19*(1-.19)/100)
```

area to the right:

```{r}
1-0.9370
```

There's about a 6.3% chance of observing such a sample.  


Ex)  At DU, 77% of students are from out-of-state.  In a sample of 60 students, what's the prob that at least half of them are from out-of-state.

    P(phat >= .5)

    p = .77
    n = 60
    
    mu_phat = .77

    sigma_phat = sqrt( .77*(1-.77)/60 )
    
Q:  Are we sure phat is normal?

```{r}
60*.77
```

```{r}
60*(1-.77)
```

Yes!  phat normal!  n is "large enough"!

z-score:

```{r}
(.5 - .77)/sqrt( .77*(1-.77)/60 )
```

Need:  P(Z >= -4.97) ~ 1

In a sample of size 60, it's nearly certain (almost 100%) that at least half of students are out-of-state students.



                   means          |          proportions
                   
center:         mu_xbar = mu_x                 mu_phat = p

spread:     sigma_xbar = sigma_x/sqrt(n)    sigma_phat = sqrt( p(1-p)/n )

z-score:  (xbar - mu_x)/(sigma_x/sqrt(n))  (phat - p)/sqrt( p(1-p)/n )

large enough     n>=30                        np>=10 and n(1-p) >= 10


# Mon April 5

Statistical Inference: the process of making mathematically supported conclusions about population paramaters based on sample statistics.

In real life, we don't know population parameters (mu, or p).  We only have sample statistics (xbar, phat).  How to use to them to say smart things about mu, p, etc?  

## Confidence Intervals

CI:  a range of reasonable values for a population paramater.

Ex:  We are 95% confident that the population proportion of all DU students who enjoy waterskiing is between 11% and 23%.

Note:  CIs are always about population parameters (mu, p, sigma, etc).  

The math:  middle percent problems!  We know the statistic, solve for the parmeter.

## Confidence Intervals for p


Ex)  In a sample of 100 DU students, 18 of them enjoyed water skiing.  Estimate the proportion of all DU students who enjoy water skiing with 95% confidence.

z-score:  +/- 1.96


    z = (phat - p)/sqrt( p(1-p)/n)
    
We know phat, let's find p!

Problem:  what's p in the denominator?  A: Use phat as approx.

Lo:  


     -1.96 = (18/100 - p)/sqrt( 18/100*(1-18/100) / 100)


```{r}
-1.96*sqrt(18/100*(1-18/100)/100) + 18/100
```

Hi:
```{r}
1.96*sqrt(18/100*(1-18/100)/100) + 18/100
```

We are 95% confident that the true population proportion of DU students who enjoy water skiing is between 10.5% and 25.5%.

## The formula

If phat follows a normal dist (np>=10 and n(1-p)>=10), then the CI for p is:


      phat +/- zstar*sqrt( phat(1-phat)/n )

where zstar (the "critical value") is the cutoff for the middle % (confidence level)

Picture:

     (----------phat----------)
     
     
The length of the "arms" is the "Margin of Error".  Ie, the stuff after +/-

In the previous example:

```{r}
1.96*sqrt(18/100*(1-18/100)/100)
```

We are 95% confident that that the true proportion is within 7.5% of our sample result.

Ex)  In Spring 2016, Prof Miller's section of 120 had 16 female students out of 25 total.

Based on this, estimate the proportion of all DU students that semester who are female with 90% confidence.

Q:  does phat follow a normal dist?

n = 25,  phat = 16/25

```{r}
25*16/25
```

```{r}
25*(1-16/25)
```

Uh-oh!  Sample size not big enough!  Not sure that phat follows a normal dist!

With caution, let's proceed.  

zstar = 1.645 (half way between 1.64 and 1.65)

Plug and chug!  phat = 16/25, zstar = 1.645

low:

```{r}
16/25 - 1.645*sqrt(16/25*(1-16/25)/25)
```

```{r}
16/25 + 1.645*sqrt(16/25*(1-16/25)/25)

```

We are 90% confident that the proportion of all DU student who are female is between 48.2% and 79.8%.

MOE:

```{r}
1.645*sqrt(16/25*(1-16/25)/25)
```

Ex)  Same data (16/25), construct 99% CI for p.

If 99% in the middle, then each tail has 0.5%.  Look up area = 0.005

zstar = 2.575 (2.57 and 2.58 are equally good).


Lo:

```{r}
16/25 - 2.575*sqrt(16/25*(1-16/25)/25)
```
Hi
```{r}
16/25 + 2.575*sqrt(16/25*(1-16/25)/25)

```

We are 99% confident that the proportion of female students is between 39.3% and 88.7%.

MOE:
```{r}
2.575*sqrt(16/25*(1-16/25)/25)
```

Bummer!  Bigger confidence means bigger MOE!





# Wed April 7

## Meaning of Confidence

Q:  What does 95% "confident" mean?  Or 90%, 99%, 80%, etc...  

Lab setup:  Virtual coin tosses.  (know p = 1/2 = 0.5)

Idea:  make some random samples of 30 coin tosses.  Use each to construct a 50% CI.

Note:  zstar  

              ---25%---50%---25%---

LOOK UP area = .25, zstar = 0.67.  For all of our intervals, the critical value is 0.67.

We see that about half of our samples generate a SUCCESSFUL confidence interval, ie an interval that REALLY DOES CONTAIN the population parameter!

If we'd constructed a 95% CI, that means 95% of the time we take a sample of size, the interval will be SUCCESSFUL.

For any one interval, there's NO WAY OF KNOWING whether the CI is correct or not!

NOT TRUE:  There's a 95% chance that our interval is "correct".  


## Relationship between Confidence and MOE

Bigger confidence means BIGGER MOE!  Bummer!  

Goal:  capture the parameter (p, mu).  The CI is like a net.  If we want to be sure that we capture our prey, we need to use a wider net!

Mathematially:  bigger confidence means bigger zstar!

Good news:  as sample size increases, MOE decreases.  Good!


In addition, we can plan in advance!  How big does n need to be?

Remember the formula (everthing after +/-)


     MOE = zstar*sqrt(phat*(1-phat)/n)

To see how large n must be, solve for n!

     n >= phat*(1-phat)(zstar/MOE)^2
     
Note:  n must be whole number (can't have n=decmial number)


Ex:  In Prof Miller's 2016 120 section, there were 16 women out of 25 students.

n = 25   phat = 16/25


In a follow-up study, we wish to estimate the true proportion of women at DU with a 95% CI and MOE no greater than 1%.


zstar:  95%  -> zstar = 1.96

desired MOE:  0.01

Plug and chug!

```{r}
(16/25)*(1-16/25)*(1.96/.01)^2
```

Oh no!  Can't take such a big sample!  Only 2200 DU students!  

Moral of the story:  adjust our expectations!  Suppose we want 90% CI with MOE no greater than 5%.

zstar: 1.645

MOE = .05

```{r}
16/25*(1-16/25)*(1.645/.05)^2
```

If we sample at least 250 DU students, then our 90% CI will have MOE no greater than 5%.


Q:  WHat if we don't have any prior data to rely on?  No guess for phat?

A:  Use phat = 0.5  (worst case scenario, ie biggest margin of error).  


Ex:  Suppose Prof Miller wishes to estimate the proportion of DU students who enjoy water skiing with 85% confidence and MOE no greater than 6%.

How many students must she sample?

No prior data, use phat = .5  (worst case scenario).

zstar:  look up 15/2 = 7.5% zstar = 1.44

Plug and chug:

```{r}
.5*(1-.5)*(1.44/.06)^2
```

Need n at least 144 to estimate p with 85% confidence and MOE no greater than 6%.


## CI for mu

Goal:  estimate population mean (mu) based on sample mean (xbar).

If n >= 30, then the ci for mu is:

     xbar +/- zstar*sigma/sqrt(n)
     

(Wait, hold on:  if we don't know mu (ergo the CI), how would know sigma???)


Ex:  In a sample of 30 DU students, the mean IQ was calculated to be 113.4.  

Based upon this, estimate the mean IQ of all DU students with 96% confidence.

Assume that sigma = 15.  (!!!)


Find crit val:  96% in the middle, then 2% in the tails. 

zstar = 2.05.  

Plug and chug!

```{r}
113.4 - 2.05*15/sqrt(30)
```

```{r}
113.4 +2.05*15/sqrt(30)

```

We are 96% confident that the true mean IQ of DU students is between 107.8 and 119.0.

## t distribution

Problem:  don't know sigma.  What to do?  

Answer:  use s, the sample stdev!

Problem:  

    z = (xbar - mu)/(sigma/sqrt(n))
    
Is normal.  BUT:

         (xbar - mu)/( s /sqrt(n)) 
         
does NOT follow a normal distribution!  


Answer:  the "t" distribution is like the normal, but accounts for fact that we don't know the pop stdev.  Based on sample stdev.

Properties of the t distribution:

- very similar to z dist.  
  - symmetric
  - centered at zero
  - bell shaped
  
- differences:
  - thicker tails!  more area at the extremes.
  - bigger critical values.  need to farter out to get the middle %. Bummer!
  - Bigger margin of error!  Bummer!  But, honest:  accounting for the fact that we 
    don't know the population stdev.  s isn't as good as sigma!
  - the specific shape depends on sample size.  Called:  "degrees of freedom".  
      
             df = n-1

   - as df increases (bigger sample), t gets closer and closer to the normal dist!
   
   
## Using the t table

Since there are many options for df, the t table works differently!  

- the values "in" the table are t-scores
- the areas are in the top margin
- df are on left margin


Ex)  What are the t-scores that cutoff the middle 95% of the dist when n=35.

   df = 34
   
tstar = 2.03.  (df = 34, one tail area = .025)

Ex)  In a sample of 40 women at DU, their mean height was computed to be 64.6", with stdev 3.1".


WARNING:  3.1" is the SAMPLE stdev, s!  use t!

Estimate the true mean height of women at DU with 99% confidence.

Need tstar:  99% in the middle, each tail has 0.5%  (.005).  df = 39

tstar = 2.71.

Plug and chug!

```{r}
64.6 - 2.71*3.1/sqrt(40)
```

```{r}
64.6 + 2.71*3.1/sqrt(40)

```

We're 99% confident that the true mean height of women at DU is between 63.3" and 65.9".

MOE:

```{r}
2.71*3.1/sqrt(40)
```

Ex)  Over the course of a year, Prof Miller ordered pizza delivery 28 times  and recorded a mean delivery time of 28.2 min with stdev 5.3 min.

Estimate the mean delivery time for her pizza with 98% confidence.  

tstar:  98% in the middle, 

   - one tail has 1%
   - two tails together have 2%
   
tstar = 2.47

Plug and chug!

lo:
```{r}
28.2 - 2.47*5.3/sqrt(28)
```
```{r}
28.2 + 2.47*5.3/sqrt(28)
```

We are 98% confident that the true mean delivery time is between 25.7" and 30.7".

## Planning for sample size in CI for mu

Idea:  solve for n in the MOE formula

     MOE = tstar*s/sqrt(n)
     
     n = (tstar*s/MOE)^2
     
Note:  need an estimate for s.
       what about tstar?  don't know n, don't know df!  
       
A:  for 95% confidence, approximate tstar = 2

# Mon April 12



Bunny rabbits have higher resting heart rate than humans, with a mean of 220 bpm and stdev of 20 bpm.  Assume that bunny heart rate follows a normal distribution.

If we take a sample of 16 bunnies, what's the probability that their mean heart rate is at least 210 bpm?

P(xbar >= 210)

zscore:

```{r}
(210 - 220)/(20/sqrt(16))
```

In Granville,  38% of homes display a flower garden in the front yard.  If we take a sample of 30 homes, what's the probability that at least half of them display a flower garden in the front yard.

Show all work, and justify why you were able to calculate this probability at all.




P(phat >= .5)

```{r}
30*.38
```
```{r}
30*(1-.38)
```
z-score:

```{r}
(.5-.38)/sqrt(.38*(1-.38)/30)
```

P(Z>1.35) = .0845


Ex)  In a large lecture section, the average time taken to complete the final exam was 113.2 min, 15.3 min.  There are 200 students in the course.

Based on this, construct a 90% CI for the true mean exam completion time.

Find tstar:  90% in middle:

   - one tail area = 5%
   - two tail area (together) = 10%
   
df = n - 1 = 200 - 1 =  199

Note:  if df not exactly on table, use next smallest.  Here:  df = 150 

tstar = 1.66

lo:

```{r}
113.2 - 1.66*15.3/sqrt(200)
```
```{r}
113.2 + 1.66*15.3/sqrt(200)

```

We are 90% confident that that the true mean completion time for this exam is between 111.4 min and 115.0 min.


# Monday April 12

## Hyp Tests


So far:

Confidence Intervals estimate a population paramater based on sample data.

Hypothesis tests answer a question/claim about a population parameter.


Ex:  We wonder if mean cholesterol is lower for patients who take new drug.

Hyp tests are criminal trials!

In a criminal trial:

1)  Q:  Innocent or guilty?  Assume by default that they're innocent, but the reason we're doing the trial is because they're think they're guilty!

2) Get evidence in the form of witness testimony and experts, etc.

3) A jury/judge decides how strong the evidence is.

4)  Conclusion:  if evidence is strong enough, then convict!  Guilty!  If evidence isnt' strong enough, can't convict.   Innocent.


Hyp test:  pop parameter is on trial!

1)  Two competing hypotheses:
    - H0:  The status quo.  Our assumption about the paramater.  Innocent!
    - Ha:  Our claim/question.  The thing we're trying to support.  Guilty!
    
    Note:
    
      - The H0 is always a statement of equality (=) about a pop parameter.
      - The Ha is always an inequality (<, >, !=).  
      
    H0 and Ha are always logical opposites.  
    
2)  The evidence is data!  Specifically:  z-score!  called "test stat"

3)  "Strength" of evidence comes from probability!  Called "p-value"

    The p-value is the probability of observing a sample result that's as (or 
    more) extreme than the one we got, assuming H0 is true!!!!
    
4)  Conclusion.  Small p-vals are evidence agains the H0.

     If p-val is small, then H0 says our data is very unlikely.
     Ie, if p-val is small, then H0 and data disagree!!
     
     If p-val is small, reject the H0!  There's strong evidence to support the
     Ha!
     
     If p-val is large, then H0 and data agree.  We did not find evidence
     to support Ha.  Boo!
     
     
## Hyp Tests for p (proportions)

Ex)  In a samle of 100 DU students, 62 have brown hair.  Does this provide strong evidence that more than half of DU students have brown hair?


Note:  phat is normal:

100*(62/100) > 10
(100)(1-62/100) > 10

1)  Hypotheses

H0:  p = .5
Ha:  p > .5


2) Test stat

z-score:

```{r}
(62/100 - .5)/sqrt(.5*(1-.5)/100)
```

3) p-val

```{r}
1-0.9918
```

p-value = .0082

4)  Q:  how small is small enough?  The "significance level" is the cutoff.  

    symbol:  alpha
    
    By default:  use alpha = .05.
    
Since p-val < alpha, reject H0.  We have strong evidence to support our claim that more than half of students have brown hair.

# Wed April 14

## Notes about hyp tests

- p-vals are always tail areas.  But:  which tail?  A:  Ha!

  If Ha has ">", then right tail.  
  If Ha has "<", then left tail.  
    
    
    
- Every conclusion has two parts:

  - If we have small p-val (p-val<alpha), we reject H0.  If p is large (>alpha), we fail to reject H0.
  - If we reject H0, then we've found strong evidence to support Ha.  Otherwise, we don't have enough evidence to support Ha.
  
WARNING:  If we fail to reject H0, this doesn't prove H0 is true.

## Significance Level

How small is small enough for p-val?  You decide!  The "cutoff" is called alpha, the significance level.  

If alpha not stated, assume alpha = 0.05.  

Ex)  Prof Miller has a proposition to change the faculty handbook.  For it to succeed, more than 2/3 of the faculty must vote to support it.  

To investigate whether there's enough support, she takes a sample of 50 facult members, and finds that 39 of them support her proposition.

Q:  Does this provide strong evidence that at least 2/3 of all faculty support her?


1) Hypotheses

H0: p=2/3
Ha: p>2/3    <- right tail test

2) test stat

```{r}
(39/50-2/3)/sqrt(2/3*(1-2/3)/50)
```

Note:  Ci for p:

     phat +/- zstar*sqrt(phat(1-phat)/n)


3) p-val

Justify:

```{r}
50*2/3
```
```{r}
50*(1-2/3)
```
Yes!  phat is normal!

left tail area = .9554

p-val:

```{r}
1-.9554
```

If H0 is true, there's a 4.46% chance of observing a sample result like the one we got.


4) conclusion

    p < alpha
0.0446 < 0.05

Reject H0.  We have strong evidence that more than 2/3 of faculty support the proposition.

## Two tail tests

So far:

Ha:  <   -> left tail test
Ha:  >   -> right tail test

Q:  what if Ha has "!="  (not equals)

A:  Then, both very large and very small results give evidence against H0.

Q:  Only one test stat, so only one tail area.  What's the other?

A:  For two tail tests, double the tail area for p-val!

Ex)  Prof Miller suspects that a dice is unfair.  In particular, she thinks the proportion of times that "3" shows up differs from 1/6.

To investigate, she rolls the dice 100 times and observes that "3" comes up 13 times.

1) Hypotheses

H0:  p  = 1/6
Ha:  p != 1/6    <- two tail test!!!

2) Test stat

```{r}
(13/100 - 1/6)/sqrt(1/6*(1-1/6)/100)
```

left-tail area:  0.1635

p-val:  2*.1635

```{r}
2*.1635
```

p-val = 0.327

4)  Conclusion


The p-val is greater than 0.05, so we fail to reject H0.  We didn't find strong evidence that the dice is unfair.  

## Hyp tests for mu (mean!)

Ex)  Prof Miller suspects that men at DU are taller, on average, then the national mean height for adult men of 69".  

To investigate, a sample is taken of 45 studnents.  Their mean height is computed to be 69.8" with stdev 2.5".

Does this provide strong evidence to support the claim?


1) Hypotheses

H0:  mu = 69
Ha:  mu > 69

2) Test stat

Note:  always use t-dist for means!

t-score:

```{r}
(69.8-69)/(2.5/sqrt(45))
```

df = 45 - 1 = 44


3) p-val

Use t table!

  .01 < p-val .025
  
  


    
    
    
    
    
# Wed April 14

## Notes about hyp tests

p-vals are always "tail areas".  But:  which tail?

Answer:  look at Ha!

Ha:  p < .25    ->  left area.  "Left tail test"

Ha:  mu > 10.3  ->  right tail area.  "Right tail test"

Conclusions:  only two possible conclusions:

   - If p<alpha (sig level), then reject H0.  
      There's strong evidence to support Ha.  (Yaay!)
   - Else if p>alpha, then fail to reject H0.
      There's not strong evidence to support Ha.  (Booooo)
      
Always two sentences:  reject or fail to reject?  Support Ha, or not?

ALways speak in context about Ha!


EX)  Prof Miller is proposing a change to the faculty handbook.  For her proposition to pass, she needs the support of at least 2/3 of the faculty.

To investigate whether or not she has enough support before the real vote, she takes a poll of 50 faculty members.  Of those, 39 are in support.

Does this provide strong evidence that 2/3 of all faculty (or more) support her?

Perform and appropriate hyp test with alpha = 0.05.

1)  Hypotheses

H0:  p=2/3
Ha:  p>2/3   <- right tail!

2)  Test stat

z-score:

```{r}
(39/50-2/3)/sqrt(2/3*(1-2/3)/50)
```

3) p-val

Note:

50*(39/50) > 10
50(1-39/50) > 10

Yep, normal dist!

```{r}
1-.9554
```

If it's true that 2/3 (or less) of the faculty support the proposal, then there's only a 4.5% chance of observing a sample proportion as large as the one we got.

4)  Here:  p<alpha

    .046 < .05
    
Reject H0!  There's strong evidence to support the claim that more than 2/3 of the faculty support the proposition.

Yay!  I found the evidence I was looking for!



## Two tail tests

So far:

Ha:   >   -> right tail
Ha:   <   -> left tail

Q:  what if Ha has !=  ?

A:  two tail test.  area could come from -either- side!  Could be bigger, could be smaller!

Want to count both tail areas!  Multiply by 2!

Ex:  Prof Miller suspects that a dice isn't fair.  She thinks the proportion of "3"s is not 1/6.

To test this, she rolls the dice 100 times, and observes that 14 of them are "3"s.  Does this provide strong evidence to support her claim that the proportion differs from 1/6?


1)  Hypotheses

H0:  p  = 1/6
Ha:  p != 1/6    <- two tail!

2)  test stat

```{r}
(14/100 - 1/6)/sqrt(1/6*(1-1/6)/100)
```


3)  p-val

left tail area:  .2358

p = 2(.2358)

```{r}
2*(.2358)
```

p = .4716 (both tail areas)

If the dice is fair (p=1/6), then there's a 47% chance of observing a sample result like this.  Pretty likely!

4)  p>alpha.  Fail to reject H0.  We did not find evidence to support Prof Miller's claim that the p!=1/6.


WARNING:  We have NOT proven that p=1/6.  Instead, we failed to find evidence that p!=1/6.  

## Hyp tests for mu (means)

So far:  tested proportions.

Now:  test means.

Ex)  Prof Miller suspects that women at DU are taller on average than women in the US as a whole.  The mean height for women in the US is 64".  

To investigate, she takes sample of 45 students and finds their average height to be 64.9", stdev 2.5".

1)  Hypotheses

H0:  mu = 64
Ha:  mu > 64


2) test stat

Note:  for means, always use t (don't know sigma!)

t-score:

```{r}
(64.9 - 64)/(2.5/sqrt(45))
```

df = 45 - 1 = 44

3) p-val:  look on t-table!

p-val is between .005 and .01.

4)  Conclusion

p<alpha.  Reject H0.  We've found strong evidence that mean height of women at DU is higher than the national average of 64". 









## 2-t tests

Idea:  How we compare two populations using two sample results?

A:  investigate mu1 - mu2  (the difference between them!)

So far, one population:  invesgtigate mu.

    estimate:  xbar 
    
    stdev:    s/sqrt(n)
    
What about 2 pops?  investigate mu1-mu2.

     estimate:  xbar1 - xbar2
     
     stdev:     sqrt( s1^2/n1 + s2^2/n2)
     
     where s1,s2 are stdevs of samples, n1,n2 are sample sizes
     
     
     
## Hyp Tests:

One mean:

    test stat = (obs-exp)/stdev
              = (xbar - mu)/(s/sqrt(n))


Two means:

    test stat = (xbar1-xbar2 - (mu1-mu2))/sqrt( s1^2/n1 + s2^2/n2)
    
    
    
## CIs:

In general:


     (observed) +/- (crit val)*(stdev)
     
For mu1-mu2:

     (xbar1-xbar2) +/- tstar*sqrt( s1^2/n1 + s2^2/n2 )
     
     
## Examples

Ex)  Prof Miller has two calc sections.  The first has 42 students, and on the final their average score was 76.3 (stdev 10.2).  The second section has 39 students, and on the final their average score was 80.1 (stdev 7.6).

Is there statistically significant evidence of a difference in mean performance between the two?

      xbar1 = 76.3   xbar2 = 80.1
      s1 = 10.2      s2    = 7.6
      n1 = 42        n2    = 39

Hypotheses:

H0:  mu1-mu2  = 0 (mu1  = mu2)  <- ALWAYS THE H0 FOR 2-T!!!!
Ha:  mu1-mu2 != 0 (mu1 != mu2)  <- two tail test!


Test stat:

```{r}
( (76.3-80.1) - 0 )/sqrt(10.2^2/42 + 7.6^2/39)
```

DF:  which to use?  Conservative approximation:  use the smaller one!

    df = 39-1 = 38
    
p-val: 

      .05 < p-val < .1    (two tail)
      
Conclusion:

Since p>.05, we fail to reject H0.  We did not find statistically significant evidence of a difference in mean performance.



Ex)  A researcher wonders if new drug lowers cholesterol.  Experiment:

The 25 people in the control group (no drug) have mean cholesterol of 160.4, stdev 10.9.

The 22 people in the experimental group (yes drug) have mean cholesterol 148.7, stdev 12.5.

Construct a 95% CI for the difference in mean cholesterol between the two groups.

crit val:  find one tail area = .025, df = 22 -1 = 21

    tstar = 2.08
    
Plug and chug!

lo:

```{r}
(160.4-148.7) - 2.08*sqrt(10.9^2/25 + 12.5^2/22)
```
hi
```{r}
160.4-148.7 + 2.08*sqrt(10.9^2/25 + 12.5^2/22)

```

We're 95% confident that the true difference in mean cholestrol between the groups is between 4.5 and 18.9.



                          4.5 < mu1 - mu2 < 18.9

We're confident that the difference is positive.  Ie, we're confident that mu1 is bigger than mu2!!





xbar1=7.5, s1 =3.33, n1=6
xbart2=10.83, 3.6, n2=6

H0:  mu1-mu2=0
Ha:  mu1-mu2!=0

Test stat:

```{r}
(7.5-10.83)/sqrt(3.33^2/6+3.6^2/6)
```

p-val:

   .1 < p < .2
   
Conclusion:  Fail to reject H0.  There's not enough evidence to support the claim that the mean is different on the weekend.   








# Wed April 28

## Paired Tests

Paired data is a collection of two quantitative variables where every measurement in the first is "paired" with one specific measurement in the second.

"Before and after".

Study:  difference in the pairs

      Expected:  mu_diff  = mean difference in pairs (pop)
      
      Observed:  dbar = mean difference in pairs (data)
      
      stdev:  s_diff/sqrt(n)
      
Hyp test statistic:

     t = (dbar - mu_diff)/(s_diff/sqrt(n))
     
CI:

     dbar +/- tstar*s_diff/sqrt(n)
     
     
Ex)  Does listening to Mozart increase IQ?  To investigate, do study with 4 participants.  Record IQ at begging, and again after a month of listening to Mozart every night for an hour.

Data:

      Before:  97  112  92  107
      AFter:   100 113  92  109
      diff:    -3   -1   0   -2
      
Dbar:

```{r}
(-3+-1+0+-2)/4
```
dbar = -1.5

s_diff = 1.291.

1)  Hypotheses

H0:  mu_diff = 0
Ha:  mu_diff < 0 

2)  Test stat

t = (-1.5 - 0)/(1.291/sqrt(4))
      
```{r}
(-1.5 - 0)/(1.291/sqrt(4))
```

df = 4-1 =3 

3) p-val

.05 < p-val < .1

4)  Since p>.05, we fail to reject H0.  There's not enough evidence to support the claim listening to Mozart increases IQ.


## Differences from 2-t tests

- Simpler
- Only work with paired data
- Higher power!  Yay!  Higher chance of successful test.


Ex)  A new drug is devloped to lower cholesterol.  Conduct a study:  measure cholesterol of 35 participants at the beginning, and again after a month of treatment.

Compute a mean difference in cholesterol of 8.4, stdev of 3.2.  

Construct a 95% CI for the mean difference.  Is there evidence of a difference?

critical value:  95% in the middle, tails have 2.5%, df = 34

   tstar = 2.03
   
Plug and chug!

lo:

```{r}
8.4 - 2.03*3.2/sqrt(35)
```
hi
```{r}
8.4+2.03*3.2/sqrt(35)

```

We're 95% confident that the true mean difference in cholesterol (before-after) is between 7.3 and 9.5.


Ie:

            7.3 < mean diff < 9.5
            
Yes!  Seems unlikely that mu_diff = 0.  We have evidence of a change in mean.  Specifically, seems like before is greater than after.


(hyp test)


```{r}
8.4/(3.2/sqrt(35))
```

## 2-prop z tests

Q:  how do we compare categories from two populations?

A:  study the difference in proportion



    observation:   phat1-phat2  (diff in samp prop)
    
    expected:       p1 - p2      (diff in pop prop, comes from H0)
    
    stdev:    sqrt( phat1(1-phat1)/n1 + phat2(1-phat2)/n2 )
    
    
Ex:  In 2010, a sample of 100 DU students contained 67 students with brown hair.  In 2020, a sample of 150 DU students contained 79 students with brown hair.

Is there evidence of a change in proportion of brown-haired student between 2010 and 2020?  Construct a 95% CI to investigate.


Critical value:  (remember:  use z for proportions, t for means)

    zstar = 1.96
    
Lo:

```{r}
67/100 - 79/150 - 1.96*sqrt(67/100*(1-67/100)/100 + 79/150*(1-79/150)/150)
```

```{r}
67/100 - 79/150 + 1.96*sqrt(67/100*(1-67/100)/100 + 79/150*(1-79/150)/150)

```

We are 95% confident that the difference in proportion is between 2.1% and 26.5%

Ie we're pretty sure

              2.1% <   p1-p2 < 26.5%
    
    
Unlikely that p1-p2=0, so YES, we found evidence of a difference in proportion of brown-haired students between the years.

H0:  p1-p2 = 0
Ha:  p1-p2 != 0

  
  
  
  
# Fri April 30


## Hyp Tests for 2 proportions

Idea:  take samples, compute phat1-phat2.  Say something smart about p1-p2.

Like before:

     H0:  p1-p2 = 0   (p1=p2)
     
Note:  since both samples come from populations with the same proportion, p, makes sense to combine them.  Called "pooled" estimate.

    phat_pooled = (total # of success)/(total sample size)
                = (x1 + x2)/(n1+n2)
                
More accurate, since bigger sample size!  Reasonable, since assume p1=p2.

    observed = phat1-phat2
    
    expected = p1 - p2     (=0)
    
    stdev = sqrt( phat_pool(1-phat_pool)/n1 + phat_pool(1-phat_pool)/n2)
    
    

Ex)  In a sample of 80 DU students, 49 of them said that pepperoni pizza was their favorite.  In a sample of 100 Oberlin students, 76 of them said that pepperoni was their favoforite.

Is there evidence of a difference in proportion of students who prefer pepperoni pizza at the two schools.

1) Hyps

H0:  p1-p2=0  (no difference)
Ha:  p1-p2!=0  (yes difference)

phat_pooled = (49+76)/(80+100) = 125/180
  
2) Test stat

```{r}
(49/80-76/100 - 0)/sqrt(125/180*(1-125/180)/80 + 125/180*(1-125/180)/100)
```

z = -2.13

3)  p-val  (proportions -> ztable!)

tail area = .0166

p-val = 2*.0166 = .0332

```{r}
2*.0166
```

4)  Since p<.05, reject H0.  We found strong evidence that the proportion of students who prefer pepperoni pizza differs between Oberlin and DU.





# Monday May 3

## Chi square Goodness of Fit (GOF) Test

Chi:  greek letter "k"

Idea:  what about a whole distribution of categories?

Ex:  The Mars Co (chocolate, MnMs) claims that the color of MnMs follows the distribution:

- 20% blue
- 15% brown
- 5% yellow
- 30% green
- 30% red

However, we suspect that the true distribution of color differs from the claim.

How can we test?

Test stat:  how different are our observations from our expectations?

   chissquare = sum( (obs-exp)^2/exp ) = sum( (O-E)^2/E )
   
We use a bag of MnMs as sample.  We count:

- 34 blue
- 20 brown
- 10 yellow
- 31 green
- 28 red

Does this provide evidence that the distribution of colors differs from Mars's statement?

Q:  what would our expectations be?  Total:

```{r}
34+20+10+31+28
```
Blue:  expect 
```{r}
.2*123
```
Brown:
```{r}
.15*123
```
Yellow:
```{r}
.05*123
```
Green:
```{r}
.3*123
```
Red:
```{r}
.3*123
```


Table:

Obs:  34   20   10   31    28
Exp: 24.6 18.45 6.15 36.9 36.9


Test:

1) Hypotheses

H0:  distribution is as claimed (all proportion are the same)
Ha:  at least one color proportion is different

2)  Test stat

chi-square stat

```{r}
(34-24.6)^2/24.6 + (20-18.45)^2/18.45 + (10-6.15)^2/6.15 + (31-36.9)^2/36.9 + (28-36.9)^2/36.9
```

3) p-val

chi square distribution:  distribution for std dev

df = #categories - 1 = 5-1 = 4

     .05 < p-val < .1
     
4)  Since p>.05, we fail to reject H0.  There's not strong evidence that the proportion of colors differs from Mars's claim.  


Ex)  Q:  Do babies prefer a particular color of toy?  


Data:  We have 100 babies for the study.  Each one is presented with four identical toys, each a different color:  blue, green, yellow, red.  We record which toy the baby reaches for first.

In our study, we observe:

19 blue, 25 green, 12 yellow, 44 red

Does this provide strong evidence that the babies have a preference?


1)  Hypotheses

H0:  no preference.  all proportions are equal.  ie, 1/4 for all.
Ha:  at least one proportion != 1/4.

Exp:  25 blue, 25 green, 25 yellow, 25 red

2) Test stat

```{r}
(19-25)^2/25+(25-25)^2/25+(12-25)^2/25+(44-25)^2/25
```

df = 4-1 = 3

    p-val < .001
    
4)  Since p<.05, we reject H0. We've found strong evidence of a color preference.  Strong evidence that the babies don't prefer each equally.

##  Test for independence


Two events A and B are independent if...

    P(A and B) = P(A)*P(B)
    
Expected counts:  formula above!  Observed:  data.

Test stat:  chisquare!

     chisquare = sum( (O-E)^2/E )
     
Ex:  The following is data about year in school and hair color:

              brown  black  other   total
    fresh      35      20     5      60
    soph       40      15     10     65
    total      75      35     15     125
    
Expected counts:

     E = (Row Total)*(Column Total)/(Total Total)
Expected:

              brown  black  other   
    fresh      36     16.8   7.2
    soph       39     18.2   7.8
          
chisquare:
```{r}
(35-36)^2/36+(20-16.8)^2/16.8+(5-7.2)^2/7.2+(40-39)^2/39+(15-18.2)^2/18.2+(10-7.8)^2/7.8
```

1) Hyps

H0: Year in school and hair color are independent
Ha: They're not (ie, a relationship between the two)

2)  Test stat

(work above) chisquare = 2.52

    df = (#rows - 1)*(#cols - 1) = (2-1)(3-1) = 2
    
3) p-val

    .2 < p-val < .3
    
4)  Since p-val > .05, we fail to reject H0.  There's not strong evidence of a dependence relationship between year in school and hair color.  Idea:  seems like they could be independent.  

Ex)  Does age affect the likelihood that one is concerned about anthropogenic climate change?  Data:

           Yes   No    Total
    20-29  45    15     60
    50-59  28    40     68
    Total  73    55     128
    
Expected:

           Yes      No    
    20-29  34.22   25.78
    50-59  38.78   29.22

chisquare:

```{r}
(45-34.22)^2/34.22 + (15-25.78)^2/25.78 + (28-38.78)^2/38.78+(40-29.22)^2/29.22
```

1) Hyps

H0:  age group and opinion about climate change are indpendent
Ha:  they're not (some dependence relationship)

2)  Test stat

chisquare = 14.88

df = (2-1)(2-1) =1 

3) p-val

   p-val < .001
   
4) Since p<.05, we reject H0.  There's strong evidence of a dependence relationship between age group and opinion on anthalsdfkjaldfjalsj.




  







Example:  how many other-haired soph would we expect?

```{r}
65*15/125
```

    
# Friday May 7

## Tests for correlation

Recall:  r tells the strength and direction of a linear relationship

If...

  - r close to 1, strong positive relationship
  - r close to 0, no linear relationship
  - r close to -1, strong neg relationship
  
Note:  r is a statistic based on sample data.  

Population version:  rho 

$$\rho$$

Goal:  say something smart about rho.

Problem:  sampling distribution for r is very complicated!

Good news:  if we assume rho=0, then much simpler!

H0:  rho = 0 (no lin rel)
Ha:  rho <,>,!= 0 (yes lin rel)

    t = r*sqrt(n-2)/sqrt(1-r^2)
    
    df = n-2
    
    
Ex)  Iris data:  sepal length vs sepal width

```{r}
cor(iris$Sepal.Length, iris$Sepal.Width)
```

```{r}
plot(iris$Sepal.Length, iris$Sepal.Width)
```

Q:  is there a lin rel btwn length and width?

1) Hyps

H0:  rho = 0
Ha:  rho != 0 

2) test stat

```{r}
-.118*sqrt(150-2)/sqrt(1-.118^2)
```

df = 150-2 = 148

3) p-val

    .1 < p-val < .2
    
4) Since p>.05, we fail to reject H0.  We have not found strong evidence of a lin rel btwn sepal length and sepal width.

Ex)  mpg data:  cty vs hwy

```{r}
plot(mpg$cty, mpg$hwy)
```

Q:  Is there evidence of a positive lin rel btwn cty mpg and hwy mpg?

```{r}
cor(mpg$cty, mpg$hwy)
```

    r = .956,   n=234
    
    
1)  Hyps

H0:  rho=0
Ha:  rho>0

2) test stat

```{r}
.956*sqrt(232)/sqrt(1-.956^2)
```

df = 232

3)  p-val

    p-val << .005
    
4) Since p<< .05, we super duper extra for realzies reject H0.  There is extra strong evidence of a positive lin rel btwn cty mpg and hwy mpg.

WARNING:  The test for rho can only show IF there's evidence of a linear rel (yes/no).  It CANNOT tell you how STRONG that relationship is.



# Monday May 10

Final window:  Friday 8am - Saturday 5pm

Sampling distributions:  

Two important statistics:

  - xbar (quant data)
  - phat (categorical data)
  
Central limit theorem:  as long as n is "big enough", then sample statistics (xbar and phat both) follow a normal distribution.

For xbar:

    expected = mu (population mean)
    stdev = sigma/sqrt(n)
    
    "big enough" = at least 30
    
    
For phat:

    expected = (pop proportion)
    stdev = sqrt(p(1-p)/n)
    
    "big enough" = both np and n(1-p) at least 10
    
    
    
Ex)  At DU, 82% of students are from out of state.  If we take a sample of 100 students, what's the probability that at least 80 of them are from out of state?

      p = .82
      
      P(phat>=80/100) = ?
      
CLT:  phat is normal.  z-score!

```{r}
(80/100 - .82)/sqrt(.82*(1-.82)/100)
```
    



Independent:

    P(A and B) = P(A)*P(B)
    
```{r}
table(mpg$cyl, mpg$drv) %>% addmargins()
```
Above:  pivot table showing counts of #cylinders and drive type.

Q:  Are the events "8 cyl" and "r" independent?

    - P(8) = 70/234
    - P(r) = 25/234
    - P(8 and r) = 21/234
    
Check

```{r}
70/234*25/234
```
```{r}
21/234
```
No!  Not equal, so not independent!!





## Metadata

"Data about data"

- what are the cases/individuals, how many?
- what are the variables, what kind of data/what units
- when/where/how data was collected








